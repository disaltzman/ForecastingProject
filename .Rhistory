View(df_weekly)
# Read in data
df <- fread(file="WeeklyHighDemand.csv") %>%
select(-7:-12) %>%
mutate(sku=as.factor(sku))
# Create weekly dataframe from daily data
df_weekly <- df %>%
group_by(sku, date = yearweek(transaction_date)) %>%
summarise(gross_units=sum(gross_units,na.rm=TRUE))
# Convert to tsibble (time series tibble) with SKU as key
df.ts <- df_weekly %>%
as_tsibble(index=date,key=sku)
# Change settings to not use scientific notation
options(scipen = 999)
# Set random seed
set.seed(401)
# Load packages
# Data manipulation.
library(tidyverse)
library(data.table)
library(here)
library(lubridate)
library(future)
library(reticulate)
library(kableExtra)
library(tsbox)
library(magrittr)
# Plots
library(ggplot2)
library(patchwork)
# Analyses
library(fpp3)
library(fable.prophet)
# Set plot themes
theme_set(theme_bw())
# Set working directory based upon location of this notebook
i_am("WeeklyHighDemandData.Rmd")
# Read in data
df <- fread(file="WeeklyHighDemand.csv") %>%
select(-7:-12) %>%
mutate(sku=as.factor(sku))
# Create weekly dataframe from daily data
df_weekly <- df %>%
group_by(sku, date = yearweek(transaction_date)) %>%
summarise(gross_units=sum(gross_units,na.rm=TRUE))
# Convert to tsibble (time series tibble) with SKU as key
df.ts <- df_weekly %>%
as_tsibble(index=date,key=sku)
# Count # of rows of data for each SKU
counts <- df.ts %>%
as_tibble() %>%
count(sku) %>%
arrange(desc(n))
# Get descriptives of the number of rows per SKU
kable(counts %>%
summarise(median=median(n),
mean=round(mean(n),digits=2),
min=min(n),
max=max(n))) %>%
kable_styling()
# Filter to only those combos with the maximum of 187 weeks
df.ts %<>%
group_by(sku) %>%
filter(n()==187)
# Perform Ljung-Box test
lb <- df.ts %>%
as_tibble() %>%
group_by(sku) %>%
summarise(test=ljung_box(gross_units,lag=12)[2])
# How many of the time-series are white noise?
lb %<>%
filter(test < 0.05) # 480 SKU's are not white noise
# Filter main dataset down to those that are not white noise
df.ts %<>%
group_by(sku) %>%
filter(sku %in% lb$sku)
# Remove large data frames to free up memory
rm(df,df_weekly)
# Create training set using data 80% of the data
train <- df.ts %>%
group_by(sku) %>%
slice_head(prop=0.8) %>%
as_tsibble(key=sku,index=date)
# Create testing set with remaining 20%
test <- df.ts %>%
group_by(sku) %>%
anti_join(train)
# Define decomposition model (Old QueBIT approach)
STLF <- decomposition_model(
STL(gross_units),
ETS(season_adjust)
)
# Fit models to the training data
plan(multisession) # use all cores available on CPU
# Fit models to the training data
plan(multisession) # use all cores available on CPU
test.models.weekly <- train %>%
model(
Mean = MEAN(gross_units),
Theta = THETA(gross_units),
ETS = ETS(gross_units),
Linear.Regression = TSLM(gross_units),
ARIMA = ARIMA(gross_units),
QueBIT_Old = STLF,
Prophet = prophet(gross_units ~ season(period="year",3))) %>%
mutate(Intermediate.Ensemble = (Theta + Linear.Regression + ETS +
ARIMA + QueBIT_Old)/5,
Kitchen.Sink.Ensemble = (Mean + Theta + Linear.Regression +
ETS + ARIMA + QueBIT_Old + Prophet)/7)
# Fit models to the training data
plan(multisession) # use all cores available on CPU
test.models.weekly <- train %>%
model(
Mean = MEAN(gross_units),
Theta = THETA(gross_units),
ETS = ETS(gross_units),
Linear.Regression = TSLM(gross_units),
ARIMA = ARIMA(gross_units),
QueBIT_Old = STLF,
Prophet = prophet(gross_units ~ season(period="year",3))) %>%
mutate(Intermediate.Ensemble = (Theta + Linear.Regression + ETS +
ARIMA + QueBIT_Old)/5,
Kitchen.Sink.Ensemble = (Mean + Theta + Linear.Regression +
ETS + ARIMA + QueBIT_Old + Prophet)/7)
test.models.weekly
# Forecast testing data
plan(multisession)
fc.test.weekly <- test.models.weekly %>%
forecast(test)
?accuracy()
View(fc.test.weekly)
test.acc.weekly <- accuracy(fc.test.weekly %>% filter(sku=="332"),df.ts %>% filter(sku=="332"))
tictoc::tic()
test.acc.weekly <- accuracy(fc.test.weekly %>% filter(sku=="332"),df.ts %>% filter(sku=="332"))
tictoc::toc()
View(test.acc.weekly)
tictoc::tic()
test.acc.weekly <- accuracy(fc.test.weekly,df.ts)
sample <- sample(df.ts$sku,240)
sample
fc.test.weekly %>% filter(sku %in% sample)
test.acc.weekly <- accuracy(fc.test.weekly %>% filter(sku %in% sample),df.ts %>% filter(sku %in% sample))
# Check accuracy of the forecasts
tictoc::tic()
test.acc.weekly <- accuracy(fc.test.weekly %>% filter(sku %in% sample),df.ts %>% filter(sku %in% sample))
sample <- sample(df.ts$sku,2)
# Check accuracy of the forecasts
tictoc::tic()
test.acc.weekly <- accuracy(fc.test.weekly %>% filter(sku %in% sample),df.ts %>% filter(sku %in% sample))
test.acc.weekly <- accuracy(fc.test.weekly %>% filter(sku %in% sample),df.ts %>% filter(sku %in% sample))
tictoc::toc()
tictoc::tic()
#test.acc.weekly <- accuracy(fc.test.weekly,df.ts)
test.acc.weekly <- accuracy(fc.test.weekly %>% filter(sku %in% sample),df.ts %>% filter(sku %in% sample))
tictoc::toc()
# Check accuracy of the forecasts
sample <- sample(df.ts$sku,3)
tictoc::tic()
#test.acc.weekly <- accuracy(fc.test.weekly,df.ts)
test.acc.weekly <- accuracy(fc.test.weekly %>% filter(sku %in% sample),df.ts %>% filter(sku %in% sample))
tictoc::toc()
# Check accuracy of the forecasts
sample <- sample(df.ts$sku,4)
tictoc::tic()
#test.acc.weekly <- accuracy(fc.test.weekly,df.ts)
test.acc.weekly <- accuracy(fc.test.weekly %>% filter(sku %in% sample),df.ts %>% filter(sku %in% sample))
tictoc::toc()
detectCores()
parallel::detectCores()
# Check accuracy of the forecasts
sample <- sample(df.ts$sku,5)
tictoc::tic()
#test.acc.weekly <- accuracy(fc.test.weekly,df.ts)
test.acc.weekly <- accuracy(fc.test.weekly %>% filter(sku %in% sample),df.ts %>% filter(sku %in% sample))
tictoc::toc()
# Check accuracy of the forecasts
tictoc::tic()
test.acc.weekly <- accuracy(fc.test.weekly,df.ts)
# Change settings to not use scientific notation
options(scipen = 999)
# Load packages.
# Data manipulation.
library(tidyverse)
library(tidytable)
library(janitor)
library(here)
library(lubridate)
library(future)
library(magrittr)
library(reticulate)
library(kableExtra)
library(tsbox)
# Plots.
library(ggplot2)
library(patchwork)
# Analyses.
library(fpp3)
library(fable.prophet)
library(modeltime)
library(modeltime.gluonts)
library(tidymodels)
library(timetk)
# Change settings to not use scientific notation
options(scipen = 999)
# Set random seed
set.seed(401)
# Load packages
# Data manipulation.
library(tidyverse)
library(data.table)
library(here)
library(lubridate)
library(future)
library(reticulate)
library(kableExtra)
library(tsbox)
library(magrittr)
# Plots
library(ggplot2)
library(patchwork)
# Analyses
library(fpp3)
library(fable.prophet)
# Set plot themes
theme_set(theme_bw())
# Change settings to not use scientific notation
options(scipen = 999)
# Set random seed
set.seed(401)
# Load packages
# Data manipulation.
library(tidyverse)
library(data.table)
library(here)
library(lubridate)
library(future)
library(reticulate)
library(kableExtra)
library(tsbox)
library(magrittr)
# Plots
library(ggplot2)
library(patchwork)
# Analyses
library(fpp3)
library(fable.prophet)
# Set plot themes
theme_set(theme_bw())
# Set working directory based upon location of this notebook
i_am("WeeklyHighDemandData.Rmd")
# Read in data
df <- fread(file="WeeklyHighDemand.csv") %>%
select(-7:-12) %>%
mutate(sku=as.factor(sku))
# Create weekly dataframe from daily data
df_weekly <- df %>%
group_by(sku, date = yearweek(transaction_date)) %>%
summarise(gross_units=sum(gross_units,na.rm=TRUE))
# Change settings to not use scientific notation
options(scipen = 999)
# Set random seed
set.seed(401)
# Load packages
# Data manipulation.
library(tidyverse)
library(data.table)
library(here)
library(lubridate)
library(future)
library(reticulate)
library(kableExtra)
library(tsbox)
library(magrittr)
# Plots
library(ggplot2)
library(patchwork)
# Analyses
library(fpp3)
library(fable.prophet)
# Set plot themes
theme_set(theme_bw())
# Set working directory based upon location of this notebook
i_am("WeeklyHighDemandData.Rmd")
# Read in data
df <- fread(file="WeeklyHighDemand.csv") %>%
select(-7:-12) %>%
mutate(sku=as.factor(sku))
# Create weekly dataframe from daily data
df_weekly <- df %>%
group_by(sku, date = yearweek(transaction_date)) %>%
summarise(gross_units=sum(gross_units,na.rm=TRUE))
# Change settings to not use scientific notation
options(scipen = 999)
# Set random seed
set.seed(401)
# Load packages
# Data manipulation.
library(tidyverse)
library(data.table)
library(here)
library(lubridate)
library(future)
library(reticulate)
library(kableExtra)
library(tsbox)
library(magrittr)
# Plots
library(ggplot2)
library(patchwork)
# Analyses
library(fpp3)
library(fable.prophet)
# Set plot themes
theme_set(theme_bw())
library(fpp3)
# Read in data
df <- fread(file="WeeklyHighDemand.csv") %>%
select(-7:-12) %>%
mutate(sku=as.factor(sku))
# Create weekly dataframe from daily data
df_weekly <- df %>%
group_by(sku, date = yearweek(transaction_date)) %>%
summarise(gross_units=sum(gross_units,na.rm=TRUE))
# Set working directory based upon location of this notebook
i_am("WeeklyHighDemandData.Rmd")
# Read in data
df <- fread(file="WeeklyHighDemand.csv") %>%
select(-7:-12) %>%
mutate(sku=as.factor(sku))
# Create weekly dataframe from daily data
df_weekly <- df %>%
group_by(sku, date = yearweek(transaction_date)) %>%
summarise(gross_units=sum(gross_units,na.rm=TRUE))
View(df)
yearweek(df$transaction_date)
# Change settings to not use scientific notation
options(scipen = 999)
# Set random seed
set.seed(401)
# Load packages
# Data manipulation.
library(tidyverse)
library(data.table)
library(here)
library(lubridate)
library(future)
library(reticulate)
library(kableExtra)
library(tsbox)
library(magrittr)
# Plots
library(ggplot2)
library(patchwork)
# Analyses
library(fpp3)
library(fable.prophet)
# Set plot themes
theme_set(theme_bw())
# Set working directory based upon location of this notebook
i_am("WeeklyHighDemandData.Rmd")
# Read in data
df <- fread(file="WeeklyHighDemand.csv") %>%
select(-7:-12) %>%
mutate(sku=as.factor(sku))
# Create weekly dataframe from daily data
df_weekly <- df %>%
group_by(sku, date = yearweek(transaction_date)) %>%
summarise(gross_units=sum(gross_units,na.rm=TRUE))
# Convert to tsibble (time series tibble) with SKU as key
df.ts <- df_weekly %>%
as_tsibble(index=date,key=sku)
# Change settings to not use scientific notation
options(scipen = 999)
# Set random seed
set.seed(401)
# Load packages
# Data manipulation.
library(tidyverse)
library(data.table)
library(here)
library(lubridate)
library(future)
library(reticulate)
library(kableExtra)
library(tsbox)
library(magrittr)
# Plots
library(ggplot2)
library(patchwork)
# Analyses
library(fpp3)
library(fable.prophet)
# Set plot themes
theme_set(theme_bw())
# Set working directory based upon location of this notebook
i_am("WeeklyHighDemandData.Rmd")
# Read in data
df <- fread(file="WeeklyHighDemand.csv") %>%
select(-7:-12) %>%
mutate(sku=as.factor(sku))
# Create weekly dataframe from daily data
df_weekly <- df %>%
group_by(sku, date = yearweek(transaction_date)) %>%
summarise(gross_units=sum(gross_units,na.rm=TRUE))
# Convert to tsibble (time series tibble) with SKU as key
df.ts <- df_weekly %>%
as_tsibble(index=date,key=sku)
# Count # of rows of data for each SKU
counts <- df.ts %>%
as_tibble() %>%
count(sku) %>%
arrange(desc(n))
# Get descriptives of the number of rows per SKU
kable(counts %>%
summarise(median=median(n),
mean=round(mean(n),digits=2),
min=min(n),
max=max(n))) %>%
kable_styling()
# Filter to only those combos with the maximum of 187 weeks
df.ts %<>%
group_by(sku) %>%
filter(n()==187)
# Perform Ljung-Box test
lb <- df.ts %>%
as_tibble() %>%
group_by(sku) %>%
summarise(test=ljung_box(gross_units,lag=12)[2])
# How many of the time-series are white noise?
lb %<>%
filter(test < 0.05) # 480 SKU's are not white noise
# Filter main dataset down to those that are not white noise
df.ts %<>%
group_by(sku) %>%
filter(sku %in% lb$sku)
# Remove large data frames to free up memory
rm(df,df_weekly)
# Create training set using data 80% of the data
train <- df.ts %>%
group_by(sku) %>%
slice_head(prop=0.8) %>%
as_tsibble(key=sku,index=date)
# Create testing set with remaining 20%
test <- df.ts %>%
group_by(sku) %>%
anti_join(train)
# Define decomposition model (Old QueBIT approach)
STLF <- decomposition_model(
STL(gross_units),
ETS(season_adjust)
)
# Fit models to the training data
plan(multisession) # use all cores available on CPU
test.models.weekly <- train %>%
model(
Mean = MEAN(gross_units),
Theta = THETA(gross_units),
ETS = ETS(gross_units),
Linear.Regression = TSLM(gross_units),
ARIMA = ARIMA(gross_units),
QueBIT_Old = STLF,
Prophet = prophet(gross_units ~ season(period="year",3))) %>%
mutate(Intermediate.Ensemble = (Theta + Linear.Regression + ETS +
ARIMA + QueBIT_Old)/5,
Kitchen.Sink.Ensemble = (Mean + Theta + Linear.Regression +
ETS + ARIMA + QueBIT_Old + Prophet)/7)
# Forecast testing data
plan(multisession)
fc.test.weekly <- test.models.weekly %>%
forecast(test)
# Check accuracy of the forecasts
tictoc::tic()
test.acc.weekly <- accuracy(fc.test.weekly,df.ts)
tictoc::tic()
test.acc.weekly <- accuracy(fc.test.weekly,df.ts)
gc()
?accuracy
remove.packages("fpp3")
remove.packages("fable")
remove.packages("fabletools")
remove.packages("feasts")
# Data manipulation.
library(tidyverse)
library(data.table)
library(here)
library(lubridate)
library(future)
library(reticulate)
library(kableExtra)
library(tsbox)
library(magrittr)
# Plots
library(ggplot2)
library(patchwork)
# Analyses
library(fpp3)
remove.packages("fpp3")
# Analyses
library(fpp3)
library(fable.prophet)
# Set working directory based upon location of this notebook
i_am("WeeklyHighDemandData.Rmd")
df <- fread(file="WeeklyHighDemand.csv") %>%
select(-7:-12) %>%
mutate(sku=as.factor(sku))
# Create weekly dataframe from daily data
df_weekly <- df %>%
group_by(sku, date = yearweek(transaction_date)) %>%
summarise(gross_units=sum(gross_units,na.rm=TRUE))
# Convert to tsibble (time series tibble) with SKU as key
df.ts <- df_weekly %>%
as_tsibble(index=date,key=sku)
