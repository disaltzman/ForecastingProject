labs(x="Model",y="MASE score (lower is better")
ggplot(best.mase,aes(x=.model_desc,y=mase)) +
geom_point()
ggplot(best.mase,aes(x=reorder(.model_desc, -plotdata$n),y=mase)) +
geom_point()
.model_desc
ggplot(best.mase,aes(x=.model_desc,y=mase)) +
geom_point()
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase)) +
geom_point()
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase)) +
geom_point() +
geom_hline(yintercept = 1,linetype="dashed")
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase)) +
geom_boxplot() +
geom_hline(yintercept = 1,linetype="dashed")
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase)) +
geom_point() +
geom_hline(yintercept = 1,linetype="dashed")
View(best.mase)
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase)) +
geom_jitter() +
geom_hline(yintercept = 1,linetype="dashed")
?geom_jitty
?geom_jitter
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase)) +
geom_jitter(width=0.25) +
geom_hline(yintercept = 1,linetype="dashed")
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase)) +
geom_jitter(width=0.20) +
geom_hline(yintercept = 1,linetype="dashed")
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase)) +
geom_jitter(width=0.20,alpha=0.7) +
geom_hline(yintercept = 1,linetype="dashed")
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase)) +
#geom_jitter(width=0.20,alpha=0.7) +
geom_violin() +
geom_hline(yintercept = 1,linetype="dashed")
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase)) +
#geom_jitter(width=0.20,alpha=0.7) +
geom_boxplot() +
geom_hline(yintercept = 1,linetype="dashed")
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase)) +
geom_jitter(width=0.20,alpha=0.7) +
geom_hline(yintercept = 1,linetype="dashed")
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,size=mase)) +
geom_jitter(width=0.20,alpha=0.7) +
geom_hline(yintercept = 1,linetype="dashed")
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase)) +
geom_jitter(width=0.20,alpha=0.7) +
geom_hline(yintercept = 1,linetype="dashed") +
geom()
View(best.mase)
View(best.mase)
best.mase %>%
group_by(.model_desc) %>%
mutate(sd=sd(mase))
best.mase %>%
group_by(.model_desc,sku) %>%
mutate(sd=sd(mase))
best.mase %>%
group_by(.model_desc) %>%
mutate(sd=sd(mase))
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,size=sd(mase))) +
geom_jitter(width=0.20,alpha=0.7) +
geom_hline(yintercept = 1,linetype="dashed")
data <- data.frame(x = rnorm(100), y = rnorm(100), group = rep(1:5, each = 20))
# Calculate standard deviation of x variable by group
sd_by_group <- tapply(data$x, data$group, sd)
# Create ggplot object
ggplot(data, aes(x = x, y = y)) +
geom_point(aes(size = sd_by_group[group])) +
scale_size(range = c(1, 10))
best.mase %>%
group_by(.model_desc) %>%
mutate(scaled_mase=scale(mase))
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,size=scale(mase))) +
geom_jitter(width=0.20,alpha=0.7) +
geom_hline(yintercept = 1,linetype="dashed")
best.mase %>%
group_by(.model_desc) %>%
mutate(scaled_mase=scale(mase))
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,size=scale(mase))) +
geom_jitter(width=0.20,alpha=0.7) +
geom_hline(yintercept = 1,linetype="dashed")
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,size=scale(mase))) +
geom_jitter(width=0.20,alpha=0.7) +
geom_hline(yintercept = 1,linetype="dashed") +
ggeasy::easy_remove_legend()
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,size=scale(mase))) +
geom_jitter(width=0.20,alpha=0.7) +
geom_hline(yintercept = 1,linetype="dashed")
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,size=scale(mase))) +
geom_jitter(width=0.20,alpha=0.7) +
geom_hline(yintercept = 1,linetype="dashed") +
scale_size("Normalized MASE score")
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,size=scale(mase))) +
geom_jitter(width=0.20,alpha=0.7) +
scale_size("Normalized MASE score")
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,size=scale(mase))) +
geom_jitter(width=0.20,alpha=0.5) +
scale_size("Normalized MASE score")
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,group=.model_desc,size=scale(mase))) +
geom_jitter(width=0.20,alpha=0.5) +
scale_size("Normalized MASE score")
best.mase %<>%
group_by(.model_desc) %>%
mutate(scaled_mase=scale(MASE))
# Calculate scaled MASE scored
best.mase %<>%
group_by(.model_desc) %>%
mutate(scaled_mase=scale(mase))
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,group=.model_desc,size=scaled_mase)) +
geom_jitter(width=0.20,alpha=0.5) +
scale_size("Normalized MASE score")
plotdata <- best.mase %>%
group_by(.model_desc) %>%
summarize(n=n(),
avg.MASE=mean(mase),
med.MASE=median(mase)) %>%
pivot_longer(cols = avg.MASE:med.MASE,
names_to="criterion",
values_to = "value")
ggplot(plotdata,aes(x=reorder(.model_desc, -n),y=value,fill=criterion,alpha=n)) +
geom_bar(stat="summary",position="dodge") +
geom_hline(yintercept = 1,linetype="dashed") +
scale_fill_manual("Criterion",labels=c("Average MASE","Median MASE"),values=c("black","orange")) +
scale_alpha("# of SKU's") +
ggeasy::easy_rotate_labels(which="x",angle=90) +
ggeasy::easy_move_legend("right") +
ggeasy::easy_text_size(14) +
labs(x="Model",y="MASE score (lower is better)")
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,group=.model_desc,size=scaled_mase)) +
geom_jitter(width=0.20,alpha=0.5) +
scale_size("Normalized MASE score") +
labs(x="Model",y="MASE score (lower is better)")
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,group=.model_desc,size=scaled_mase)) +
geom_jitter(width=0.20,alpha=0.5) +
scale_size("Normalized MASE score") +
labs(x="Model",y="MASE score (lower is better)") +
ggeasy::easy_all_text_size(14)
# Calculate scaled MASE scored
best.mase %<>%
group_by(.model_desc) %>%
mutate(scaled_mase=scale(mase))
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,group=.model_desc,size=scaled_mase)) +
geom_jitter(width=0.20,alpha=0.5) +
scale_size("Normalized MASE score") +
labs(x="Model",y="MASE score (lower is better)") +
ggeasy::easy_all_text_size(14)
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,group=.model_desc,size=scaled_mase)) +
geom_jitter(width=0.20,alpha=0.5) +
scale_size("Normalized \n MASE score") +
labs(x="Model",y="MASE score (lower is better)") +
ggeasy::easy_all_text_size(14)
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,group=.model_desc,size=scaled_mase)) +
geom_jitter(width=0.20,alpha=0.5) +
scale_size("Normalized \nMASE score") +
labs(x="Model",y="MASE score (lower is better)") +
ggeasy::easy_all_text_size(14)
# Calculate scaled MASE scored
best.mase %<>%
group_by(.model_desc) %>%
mutate(scaled_mase=scale(mase))
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,group=.model_desc,size=scaled_mase)) +
geom_jitter(width=0.20,alpha=0.5) +
scale_size("Normalized \nMASE score") +
labs(x="Model",y="MASE score (lower is better)") +
ggeasy::easy_all_text_size(14)
ggplot(plotdata,aes(x=reorder(.model_desc, -n),y=value,fill=criterion,alpha=n)) +
geom_bar(stat="summary",position="dodge") +
geom_hline(yintercept = 1,linetype="dashed") +
scale_fill_manual("Criterion",labels=c("Average MASE","Median MASE"),values=c("black","orange")) +
scale_alpha("# of SKU's") +
ggeasy::easy_rotate_labels(which="x",angle=90) +
ggeasy::easy_text_size(14) +
labs(x="Model",y="MASE score (lower is better)")
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,group=.model_desc,size=scaled_mase)) +
geom_jitter(width=0.20,alpha=0.5) +
scale_size("Normalized \nMASE score") +
labs(x="Model",y="MASE score (lower is better)") +
ggeasy::easy_all_text_size(14) +
ggeasy::easy_rotate_labels(which="x",angle=90)
# Calculate scaled MASE scored
best.mase %<>%
group_by(.model_desc) %>%
mutate(scaled_mase=scale(mase))
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,group=.model_desc,size=scaled_mase)) +
geom_jitter(width=0.20,alpha=0.5) +
scale_size("Normalized \nMASE score") +
labs(x="Model",y="MASE score (lower is better)") +
ggeasy::easy_all_text_size(14) +
ggeasy::easy_rotate_labels(which="x",angle=90)
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,group=.model_desc,size=scaled_mase)) +
geom_jitter(width=0.20,alpha=0.5) +
scale_size("Normalized \nMASE score") +
labs(x="Model",y="MASE score (lower is better)") +
ggeasy::easy_all_text_size(14) +
ggeasy::easy_remove_legend() +
ggeasy::easy_rotate_labels(which="x",angle=90)
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,group=.model_desc,size=scaled_mase)) +
geom_jitter(width=0.20,alpha=0.5) +
scale_size("Normalized \nMASE score") +
labs(x="Model",y="MASE score (lower is better)") +
ggeasy::easy_all_text_size(14) +
ggeasy::easy_rotate_labels(which="x",angle=90)
# Split data into training/test
FORECAST_HORIZON <- 37 # 20% of total weeks
# Split training and test
splits <- time_series_split(df_weekly,
assess=FORECAST_HORIZON,
cumulative=T)
# Plot split
splits %>%
tk_time_series_cv_plan() %>%
plot_time_series_cv_plan(.date_var = date,
.value=gross_units,
.interactive=F)
# Create recipe for feature engineering deep learning models
# Step 1. Create timeseries features from date column
# Step 2. Remove some nonsensical vectors, like those for the minute/hour of the day (more granular than original data)
# Step 3. One-Hot encode any non-numeric variable (like month name)
# Step 4. Remove zero-variance features
recipe_spec_deeplearning <- recipe(gross_units ~ .,
data = df_weekly) %>%
step_timeseries_signature(date) %>%
step_rm(contains("iso"), contains("minute"), contains("hour"),
contains("am.pm"), contains("xts")) %>%
step_dummy(all_ordered_predictors(), one_hot = TRUE) %>%
step_zv(all_predictors())
# Prep recipe (gives overview of what will be applied to training data)
prep(recipe_spec_deeplearning)
# Create model specifications for N-BEATS model
model_spec_nbeats <- nbeats(
id                    = "sku",
freq                  = "W",
prediction_length     = FORECAST_HORIZON,
epochs                = 10,
scale                 = T,
loss_function         = "MASE",
) %>%
set_engine("gluonts_nbeats")
# Create model specifications for DeepAR model
model_spec_deepAR <- deep_ar(
id                    = "sku",
freq                  = "W",
prediction_length     = FORECAST_HORIZON,
epochs                = 10,
scale                 = T,
) %>%
set_engine("gluonts_deepar")
# Create model specifications for N-BEATS ensemble model
model_spec_nbeats_ensemble <- nbeats(
id                    = "sku",
freq                  = "W",
prediction_length     = FORECAST_HORIZON,
epochs                = 10,
scale                 = T,
loss_function         = "MASE"
) %>%
set_engine("gluonts_nbeats_ensemble")
# Create model specifications for feed-forward NN with autogression
model_spec_nnetar <- nnetar_reg(hidden_units = 5
) %>%
set_engine("nnetar", MaxNWts=5000) # need to increase the max number of weights
# Create workflow for N-BEATS model
workflow_nbeats <- workflow() %>%
add_recipe(recipe_spec_deeplearning) %>%
add_model(model_spec_nbeats)
# Create workflow for DeepAR model
workflow_deepAR <- workflow() %>%
add_recipe(recipe_spec_deeplearning) %>%
add_model(model_spec_deepAR)
# Create workflow for N-BEATS Ensemble model
workflow_nbeats_ensemble <- workflow() %>%
add_recipe(recipe_spec_deeplearning) %>%
add_model(model_spec_nbeats_ensemble)
# Create workflow for feed-forward NN
workflow_nnetar <- workflow() %>%
add_recipe(recipe_spec_deeplearning) %>%
add_model(model_spec_nnetar)
# Change settings to not use scientific notation
options(scipen = 999)
# Set random seed
set.seed(401)
# Load packages.
# Data manipulation.
library(tidyverse)
library(janitor)
library(here)
library(lubridate)
library(future)
library(magrittr)
library(reticulate)
library(kableExtra)
library(tsbox)
# Plots.
library(ggplot2)
library(patchwork)
# Analyses.
library(fpp3)
library(modeltime)
library(modeltime.gluonts)
library(tidymodels)
library(timetk)
knitr::opts_chunk$set(dev.args = list(png = list(type = "cairo")))
knitr::opts_chunk$set(dpi=300)
# Set working directory to file location
i_am("2A_NBEATS.Rmd")
# Read in data
df <- data.table::fread(file="WeeklyHighDemand.csv") %>%
select(-7:-12) %>%
mutate(sku=as.factor(sku),
transaction_date=as.Date(transaction_date)) %>%
as_tibble()
# Create weekly dataframe from daily data
df_weekly <- df %>%
group_by(sku,date=yearweek(transaction_date)) %>%
summarise(gross_units=sum(gross_units,na.rm=TRUE))
# Filter to only those combos with the maximum of 187 weeks
df_weekly %<>%
group_by(sku) %>%
filter(n()==187)
# Perform Ljung-Box test
lb <- df_weekly %>%
group_by(sku) %>%
summarise(test=ljung_box(gross_units,lag=52)[2])
# How many of the time-series are white noise?
lb %<>%
filter(test < 0.05) # 446 SKU's are not white noise
# Filter main dataset down to those that are not white noise
df_weekly %<>%
group_by(sku) %>%
filter(sku %in% lb$sku)
# Convert date vector back to base R "date" type
df_weekly %<>%
mutate(date=as.Date(date)) %>%
ungroup()
# Remove large data frames to free up memory
rm(df,df.ts)
# Split data into training/test
FORECAST_HORIZON <- 37 # 20% of total weeks
# Split training and test
splits <- time_series_split(df_weekly,
assess=FORECAST_HORIZON,
cumulative=T)
# Plot split
splits %>%
tk_time_series_cv_plan() %>%
plot_time_series_cv_plan(.date_var = date,
.value=gross_units,
.interactive=F)
# Create recipe for feature engineering deep learning models
# Step 1. Create timeseries features from date column
# Step 2. Remove some nonsensical vectors, like those for the minute/hour of the day (more granular than original data)
# Step 3. One-Hot encode any non-numeric variable (like month name)
# Step 4. Remove zero-variance features
recipe_spec_deeplearning <- recipe(gross_units ~ .,
data = df_weekly) %>%
step_timeseries_signature(date) %>%
step_rm(contains("iso"), contains("minute"), contains("hour"),
contains("am.pm"), contains("xts")) %>%
step_dummy(all_ordered_predictors(), one_hot = TRUE) %>%
step_zv(all_predictors())
# Prep recipe (gives overview of what will be applied to training data)
prep(recipe_spec_deeplearning)
# Create model specifications for N-BEATS model
model_spec_nbeats <- nbeats(
id                    = "sku",
freq                  = "W",
prediction_length     = FORECAST_HORIZON,
epochs                = 10,
scale                 = T,
loss_function         = "MASE",
) %>%
set_engine("gluonts_nbeats")
# Create model specifications for DeepAR model
model_spec_deepAR <- deep_ar(
id                    = "sku",
freq                  = "W",
prediction_length     = FORECAST_HORIZON,
epochs                = 10,
scale                 = T,
) %>%
set_engine("gluonts_deepar")
# Create model specifications for N-BEATS ensemble model
model_spec_nbeats_ensemble <- nbeats(
id                    = "sku",
freq                  = "W",
prediction_length     = FORECAST_HORIZON,
epochs                = 10,
scale                 = T,
loss_function         = "MASE"
) %>%
set_engine("gluonts_nbeats_ensemble")
# Create model specifications for feed-forward NN with autogression
model_spec_nnetar <- nnetar_reg(hidden_units = 5
) %>%
set_engine("nnetar", MaxNWts=5000) # need to increase the max number of weights
# Create workflow for N-BEATS model
workflow_nbeats <- workflow() %>%
add_recipe(recipe_spec_deeplearning) %>%
add_model(model_spec_nbeats)
# Create workflow for DeepAR model
workflow_deepAR <- workflow() %>%
add_recipe(recipe_spec_deeplearning) %>%
add_model(model_spec_deepAR)
# Create workflow for N-BEATS Ensemble model
workflow_nbeats_ensemble <- workflow() %>%
add_recipe(recipe_spec_deeplearning) %>%
add_model(model_spec_nbeats_ensemble)
# Create workflow for feed-forward NN
workflow_nnetar <- workflow() %>%
add_recipe(recipe_spec_deeplearning) %>%
add_model(model_spec_nnetar)
# Fit N-BEATS model
parallel_start(16)
model_fit_nbeats <- workflow_nbeats %>%
fit(data=training(splits))
# Fit DeepAR model
model_fit_deepAR <- workflow_deepAR %>%
fit(data=training(splits))
# Fit N-BEATS Ensemble model
model_fit_nbeats_ensemble <- workflow_nbeats_ensemble %>%
fit(data=training(splits))
# Fit feed-forward NN model
model_fit_nnetar <- workflow_nnetar %>%
fit(data=training(splits))
# Fit Prophet model
model_fit_prophet <- prophet_reg(seasonality_daily = F) %>%
set_engine("prophet") %>%
fit(gross_units ~ date, training(splits))
# Fit ARIMA model
model_fit_ARIMA <- arima_reg() %>%
set_engine("auto_arima") %>%
fit(gross_units ~ date, training(splits))
# Fit ETS model
model_fit_ETS <- exp_smoothing() %>%
set_engine("ets") %>%
fit(gross_units ~ date, training(splits))
# Fit Theta model
model_fit_theta <- exp_smoothing() %>%
set_engine("theta") %>%
fit(gross_units ~ date, training(splits))
# Model fit TBATS
model_fit_TBATS <- seasonal_reg() %>%
set_engine("tbats") %>%
fit(gross_units ~ date, training(splits))
# Model fit Linear Regression
model_fit_lingreg <- linear_reg() %>%
set_engine("lm") %>%
fit(gross_units ~ date, training(splits))
parallel_stop()
# Add all fitted models to a table so that operations can be performed on all models at once
models_tbl <- modeltime_table(
model_fit_nbeats,
model_fit_deepAR,
model_fit_nbeats_ensemble,
model_fit_nnetar,
model_fit_prophet,
model_fit_ARIMA,
model_fit_ETS,
model_fit_theta,
model_fit_TBATS,
model_fit_lingreg
)
# Calculate accuracy of models on testing data
calibration_tbl <- models_tbl %>%
modeltime_calibrate(
new_data = testing(splits),
id = "sku")
# Calculate accuracy on testing data
test.acc <- calibration_tbl %>%
modeltime_accuracy(acc_by_id = TRUE)
# Filter down to models with best MASE score by SKU
best.mase <- test.acc %>%
group_by(sku) %>%
filter(mase==min(mase)) %>%
mutate(.model_desc = case_when(
grepl("TBATS",.model_desc)~"TBATS",
grepl("ARIMA",.model_desc)~"ARIMA",
grepl("NNAR",.model_desc)~"Feed For. NN",
grepl("ETS",.model_desc)~"ETS",
grepl("LM",.model_desc)~"Linear Regression",
TRUE ~ .model_desc
))
# Table
kable(best.mase %>%
group_by(.model_desc) %>%
summarize(n=n(),
avg.MASE=mean(mase),
med.MASE=median(mase)) %>%
arrange(-n),
col.names = c('Model', 'Num. of SKUs it was best for', 'Average MASE','Median MASE'),
caption="Average and Median MASE score for best fitting models") %>% kable_styling(full_width = F)
plotdata <- best.mase %>%
group_by(.model_desc) %>%
summarize(n=n(),
avg.MASE=mean(mase),
med.MASE=median(mase)) %>%
pivot_longer(cols = avg.MASE:med.MASE,
names_to="criterion",
values_to = "value")
ggplot(plotdata,aes(x=reorder(.model_desc, -n),y=value,fill=criterion,alpha=n)) +
geom_bar(stat="summary",position="dodge") +
geom_hline(yintercept = 1,linetype="dashed") +
scale_fill_manual("Criterion",labels=c("Average MASE","Median MASE"),values=c("black","orange")) +
scale_alpha("# of SKU's") +
ggeasy::easy_rotate_labels(which="x",angle=90) +
ggeasy::easy_text_size(14) +
labs(x="Model",y="MASE score (lower is better)")
save.image("C:/Users/disal/Desktop/GitHubRepos/ForecastingProject/WeeklyHighDemand/2A_NBEATS_env.RData")
