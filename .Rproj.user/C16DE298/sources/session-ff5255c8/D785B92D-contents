---
title: "DeepAR & N-BEATS with 2A Weekly Data"
output:
  html_notebook:
    toc: true
editor_options: 
  chunk_output_type: inline
---

```{r message=FALSE, warning=FALSE, include=FALSE}
# Change settings to not use scientific notation
options(scipen = 999)

# Load packages.
# Data manipulation.
library(tidyverse)
library(tidytable)
library(janitor)
library(here)
library(lubridate)
library(future)
library(magrittr)
library(reticulate)
library(kableExtra)
library(tsbox)

# Plots.
library(ggplot2)
library(patchwork)

# Analyses.
library(fpp3)
library(fable.prophet)
library(modeltime)
library(modeltime.gluonts)
library(tidymodels)
library(timetk)
```

## Overview

I loaded in the data and aggregated to the weekly level from the daily level and performing a Ljung-Box test on the combos that had a full 187 weeks data.
457 combos survived these constraints.

What is important to note is that this package, `modeltime`, by default fits statistical models like ARIMA and ETS at the global level.
This is opposed to iteratively fitting each model to each combo with potentially different parameters for each combo, as I have been doing using the package `Fable`.
This is not a problem for the deep learning models like N-BEATS and DeepAR (they do not require this kind of iterative refitting for each combo), but means that accuracy for ARIMA, Theta, ETS, etc might be lower here than previous.
The trade-off for lower accuracy is increased speed.

```{r message=FALSE, warning=FALSE, include=FALSE}
i_am("2A_NBEATS.Rmd")
```

```{r Process data, echo=TRUE, message=FALSE, warning=FALSE}
# Read in data
df <- fread.(file="2Adata.csv") %>% 
  select.(1:3) %>% 
  mutate.(transaction_date=as.Date(transaction_date),
          sku=as.factor(sku))


# Create weekly dataframe from daily data
df_weekly <- df %>% 
  group_by(sku,week=yearweek(transaction_date)) %>% 
  summarise(gross_units=sum(gross_units,na.rm=TRUE))

# Filter to only complete combos
df_weekly %<>% 
  filter.(n()==187,.by=sku)

lb <- df_weekly %>% 
  summarise.(test=ljung_box(gross_units,h=8)[2],
            .by=sku)

# How many of the time-series are white noise
lb %<>% filter.(test < 0.05) # 241 are white-noise, 457 have some predictable quality

# Filter main dataset down to those that are not white noise
df_weekly %<>% 
  filter.(sku %in% lb$sku,
          .by=sku) %>% 
  mutate.(date=as.Date(week),sku=as.factor(sku)) %>% 
  select.(-week)

# Remove large data frames to free up memory
rm(df)
```

## N-BEATS & DeepAR Compared with Statistical Models

I chose a 90/10 split with the data to better match the CNN that Tyler is programming.
I then fit an N-BEATS model with 10 epochs to the zero-centered data, an N-BEATS Ensemble model (which fits three models with different lookback lengths, and a DeepAR model with similar parameters. Everything else I left as the default settings, including the lookback length which is 2x the forecast horizon. I also fit a Prophet model, ARIMA, ETS, Theta, Naive, Seasonal Naive, and TBATS (another model for complex seasonality).
Because these models are being fit to all of the data at once, I figure that normalizing the data within combos to a mean of 0 and an SD of 1 would possibly increase accuracy (now the one combo that sold thousands of units per week isn't a massive outlier).

```{r prep train/test data, echo=TRUE, message=FALSE, warning=FALSE}
# Split data into training/test
FORECAST_HORIZON <- 19 # 10% of total weeks

# Normalize data
df_weekly %<>% 
  group_by(sku) %>% 
  mutate(gross_units_std = standardize_vec(gross_units))

# Split training and test
splits <- time_series_split(df_weekly,
                            assess=FORECAST_HORIZON,
                            cumulative=T)
# Plot split
splits %>% 
  tk_time_series_cv_plan() %>% 
  plot_time_series_cv_plan(.date_var = date,
                           .value=gross_units_std,
                           .interactive=F)

```

```{r Run models, echo=TRUE, message=FALSE, warning=FALSE}

# Fit N-BEATS Model
model_fit_nbeats <- nbeats(
  id                    = "sku",
  freq                  = "W",
  prediction_length     = FORECAST_HORIZON,
  epochs                = 10,
  scale                 = F,
  loss_function         = "MASE",
) %>%
  set_engine("gluonts_nbeats") %>%
  fit(gross_units_std ~ date + sku, training(splits))

model_fit_nbeats_ensemble <- nbeats(
  id                    = "sku",
  freq                  = "W",
  prediction_length     = FORECAST_HORIZON,
  epochs                = 10,
  scale                 = F,
  loss_function         = "MASE"
) %>%
  set_engine("gluonts_nbeats_ensemble") %>%
  fit(gross_units_std ~ date + sku, training(splits))

# Fit DeepAR model
model_fit_deepAR <- deep_ar(
  id                    = "sku",
  freq                  = "W",
  prediction_length     = FORECAST_HORIZON,
  epochs                = 10,
  scale                 = F,
) %>%
  set_engine("gluonts_deepar") %>%
  fit(gross_units_std ~ date + sku, training(splits))

# detach("package:modeltime.gluonts", unload = TRUE)
# library(neuralprophet)
# 
# model_fit_nprophet <- neural_prophet(
#     freq            = "W",
#     growth          = "linear",
#     trend_reg       = 3,
#     learn_rate      = 0.1,
#     epochs          = 10,
#     changepoint_range = 0.8,
#     seasonality_mode = "additive",
#     seasonality_daily = F
# ) %>%
#     set_engine("prophet") %>%
#     fit(gross_units ~ date, training(splits))
# 
parallel_start(12)

# Fit Prophet model 
model_fit_prophet <- prophet_reg(seasonality_daily = F) %>% 
  set_engine("prophet") %>%
  fit(gross_units_std ~ date, training(splits))

# Fit ARIMA model
model_fit_ARIMA <- arima_reg() %>%
  set_engine("auto_arima") %>%
  fit(gross_units_std ~ date, training(splits))

# Fit ETS model
model_fit_ETS <- exp_smoothing() %>%
  set_engine("ets") %>% 
  fit(gross_units_std ~ date, training(splits))

# Fit Theta model
model_fit_theta <- exp_smoothing() %>%
  set_engine("theta") %>% 
  fit(gross_units_std ~ date, training(splits))

# Model fit TBATS
model_fit_TBATS <- seasonal_reg() %>% 
  set_engine("tbats") %>% 
  fit(gross_units_std ~ date, training(splits))

# ---- MODELTIME TABLE ----

models_tbl <- modeltime_table(
    model_fit_nbeats,
    model_fit_nbeats_ensemble,
    model_fit_deepAR,
    model_fit_prophet,
    model_fit_ARIMA,
    model_fit_ETS,
    model_fit_theta,
    model_fit_naive,
    model_fit_snaive,
    model_fit_TBATS
)

# ---- CALIBRATE ----

calibration_tbl <- models_tbl %>% 
    modeltime_calibrate(
        new_data = testing(splits),
        id = "sku"
    )
```

Next I filtered the accuracy output to just the best fitting model per combo, and then created a table showing the model name, the average MASE score, and the number of combos for which it was the best model.

```{r Accuracy, echo=FALSE, message=FALSE, warning=FALSE}

# ---- ACCURACY ----

test.acc <- calibration_tbl %>%
    modeltime_accuracy(acc_by_id = TRUE)

best.mase <- test.acc %>% 
  group_by(sku) %>% 
  filter(mase==min(mase))

kable(best.mase %>% 
  group_by(.model_desc) %>% 
  summarize(n=n(),avg.MASE=mean(mase)) %>% 
  arrange(-n),caption="Average MASE score for best fitting models") %>% kable_styling(full_width = F)
```

The DeepAR model did very well here, though every model had an average MASE below 1, which is a pretty good sign.
DeepAR, N-BEATS, and the Naive model were the real winners, with everything else still performing pretty well but for fewer combos.
Accuracy for N-BEATS changes each time you rerun this analysis (while DeepAR consistently performs better than the other methods for the most number of combos), and is dependent to some extent on the number of epochs.
I had tried 50 epochs, but accuracy was seemingly consistently higher with a lower number.
That is likely because, as Tyler pointed out to me, there's no way of stopping the model fitting procedure early based on validation loss, which is a common way of avoiding selecting an overfit model (which may be occurring here).

This analysis is definitely more of a proof of concept than a definitive conclusion about N-BEATS - the wrapper that I am using in R calls `GluonTS`, Amazon's open source time-series analysis Python library, but is meant to be more plug and play than `GluonTS` and therefore streamlines a lot of what is going on in the underlying Python code.
This means I can't easily add a custom callback to allow for early stopping based on validation loss.
At a production level, a custom written PyTorch implementation would be preferable.

On a side note, the statistical methods here (ARIMA, Theta, TBATS, ETS, and Prophet) are all being applied globally and not on each combo separately as I have been doing, which limits their accuracy.
They did fit very quickly, taking at most a few minutes total compared to the \~35 minutes it took to run the same analyses iteratively in `Fable`.
If there aren't native Java implementations of the statistical analyses (which apparently is much faster than Python or R), then this is something for the Euclid team to consider.

## Comparison with Fable statistical models

To get a better sense of whether DeepAR/N-BEATS are really outperforming statistical methods, I used `Fable` to fit the various statistical models we have been using to the same weekly data.
In this case, Fable will change model parameters on a by-combo basis, which will give the statistical models a fairer shot compared to DeepAR and N-BEATS.

```{r Fable version, echo=TRUE, message=FALSE, warning=FALSE}

# Make tsibble
df.ts.weekly <- df_weekly %>% 
  as_tsibble(key=sku,index=date) %>% 
  mutate(date=yearweek(date))

# Create training set using data from 2015 to 2018
train <- df.ts.weekly %>% 
  group_by(sku) %>% 
  slice_head(prop=0.9)

# Fit models to the training data, including a "mixed" model which is just an average of the predictions of the models.
plan(multisession)

# Define decomposition model
STLF <- decomposition_model(
    STL(gross_units),
    ETS(season_adjust)
  )

# Fit models and ensembles
tictoc::tic()
test.models <- train %>% 
  model(
    mean = MEAN(gross_units),
    naive = NAIVE(gross_units),
    snaive = SNAIVE(gross_units),
    theta = THETA(gross_units),
    ets = ETS(gross_units),
    linreg = TSLM(gross_units),
    arima = ARIMA(gross_units),
    stlf = STLF,
    prophet = prophet(gross_units ~ season(period="year",3))) %>%  
  mutate(simple_ensemble = (mean + naive + snaive)/3,
         intermediate_ensemble = (theta + linreg + ets + arima + stlf)/5,
         kitchen_sink_ensemble = (mean + naive + snaive + theta + linreg + ets + arima + stlf + prophet)/9)
tictoc::toc()

# Calculate training accuracy
train.acc <- fabletools::accuracy(test.models)

# Forecast
plan(multisession,workers=12)

test <- df.ts.weekly %>%
  group_by(sku) %>% 
  slice_tail(prop=0.1)

tictoc::tic()
fc.test <- test.models %>% 
  forecast(test)
tictoc::toc()

# Check accuracy of the forecasts
tictoc::tic()
fable.test.acc <- fabletools::accuracy(fc.test,df.ts.weekly)
tictoc::toc()

best.test <- test.acc %>%
  filter.(MASE==min(MASE),
          .by=sku)

lowest.mase <- best.test %>% 
  group_by(.model) %>% 
  summarize(n=n(),avg.MASE=mean(MASE)) %>% 
  arrange(-n)
```

I then calculated the accuracy from the statistical models fit with `Fable` and compared them with the neural network models fit with `Modeltime`:

```{r comparison, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr,quietly = T)

kables(list(
  kable(lowest.mase,caption="Average MASE from iterative method on statistical models") %>% kable_styling(full_width = F),
kable(best.mase %>% 
  group_by(.model_desc) %>% 
  summarize(n=n(),avg.MASE=mean(mase)) %>% 
  arrange(-n),caption="Average MASE for global models") %>% kable_styling(full_width = F)))
```

While I'm sure DeepAR and N-BEATS could be further tuned, they were both pretty handily beaten by Prophet, the naive model, and Theta when it comes to average MASE scores.
These models don't really require any user input (other than to disable daily/weekly seasonality when it isn't possible in Prophet), and could be less computationally intensive if the number of combos is limited.
It did take quite a bit longer to run the iterative process that yielded lower MASE scores though, which is important to consider if a client has a larger dataset.
