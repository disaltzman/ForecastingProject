---
title: "Intermittent Demand Analysis with Full Bridges Data"
output:
  html_notebook:
   toc: yes
editor_options: 
  chunk_output_type: inline
---

```{=html}
<style type="text/css">
.table {

    width: 100%;

}
</style>
```

------------------------------------------------------------------------

```{r load libraries, message=FALSE, warning=FALSE, include=FALSE}
# Change settings to not use scientific notation
options(scipen = 999)

# Load packages.
# Data manipulation.
library(tidyverse)
library(tidytable)
library(janitor)
library(here)
library(lubridate)
library(future)
library(magrittr)
library(reticulate)
library(kableExtra)
library(tsbox)

# Plots.
library(ggplot2)
library(patchwork)

# Analyses.
library(fpp3)
library(fable.prophet)
library(fasster)
```

## Data importation and cleaning

I read in full Bridges dataset, which comprises 823 unique combos.
I again removed the future values and also changed the names of the combos to just be a numerical value from 1 to 823.
There were also some negative values for units which I replaced with zeroes.

```{r read in data, echo=TRUE, message=FALSE, warning=FALSE}
# Read in data
i_am("Intermittent_Demand_Analysis.Rmd")
df <- read_csv(here("BridgesForecastInput.csv"))

# Clean up "Units" vector by changing "NULL" to NA
df %<>% na_if("NULL") %>% 
  mutate.(UNITS=as.numeric(UNITS))

# Create single time vector (necessary for the package 'Fable' as it requires a single time vector instead of month and year in separate columns)
df$DATE <- make_date(df$CAL_YEAR,df$CAL_MONTH) %>% yearmonth()

# Drop SQL duplicated issues
df %<>% filter.(are_duplicated(df,index=DATE,key=COMBO_SPLIT)==F)

# Rename COMBO_SPLITS to numberic values in order
df$COMBO_SPLIT <- as.factor(df$COMBO_SPLIT)
levels(df$COMBO_SPLIT) <- 01:0823

df %<>% 
  mutate.(COMBO_SPLIT=as.character(df$COMBO_SPLIT)) %>% 
  mutate.(COMBO_SPLIT=str_pad(df$COMBO_SPLIT, 3, pad = "0")) # pad with zero for ordering purposes

# Convert to tsibble
df.ts <- df %>% as_tsibble(index=DATE,key=COMBO_SPLIT)

# Remove any observations after March 2022 (as they are all null)
df.ts %<>% filter_index(~"2022 Mar")

# Change negative UNITS values to 0
df.ts$UNITS <- ifelse(df.ts$UNITS < 0,0,df.ts$UNITS)

# Limit analysis to combos with at least 30 months data
df.ts %<>% 
  as_tibble() %>% 
  group_by(COMBO_SPLIT) %>% 
  filter(n() > 30)
```

I next ran Tyler's Demand Categories script, and as expected, most of the combos are C1:

```{r Add demand categories, echo=FALSE, message=FALSE, warning=FALSE}
source_python("../forecast_categories_v1.py")

df.ts %<>% 
  as_tibble() %>% 
  select.(COMBO_SPLIT,DATE,UNITS) %>% 
  mutate.(DATE=as.Date(DATE),OFFSET=as.numeric(DATE-today())) %>%
  rename.(CAL_PERIOD=DATE)
  
df.ts <- forecast_plots(df.ts)

# Convert dataframe back to tsibble
df.ts %<>% 
  mutate.(CAL_PERIOD=yearmonth(as.Date(CAL_PERIOD))) %>% 
  as_tsibble(key=COMBO_SPLIT,index=CAL_PERIOD)

kable(df.ts %>% 
  as_tibble() %>% 
  group_by(DEMAND_CATEGORY) %>%
  summarise(n=n_distinct(COMBO_SPLIT))) %>% kable_styling(full_width = F)
```

Now that we have the demand categories, I performed the Ljung-Box test and also generated a table showing which combos survived the test and which demand category they belonged to.
There's likely no relationship between demand categories and the Ljung-Box test but I thought it was worth looking at just in case.

```{r Ljung-Box Test, echo=FALSE, message=FALSE, warning=FALSE}
lb <- df.ts %>% 
  summarise.(test=ljung_box(UNITS,lag=24)[2],
            .by=COMBO_SPLIT)

# How many of the time-series are white noise
lb %<>% filter.(test < 0.05) # 170 are not white noise

# Create data frames for white noise and non-white noise combos
df.ts.good <- df.ts %>%  
  filter.(COMBO_SPLIT %in% lb$COMBO_SPLIT,
          .by=COMBO_SPLIT) %>% 
  as_tsibble(key=COMBO_SPLIT,index=CAL_PERIOD)

df.ts.bad <- df.ts %>%  
  filter.(!COMBO_SPLIT %in% lb$COMBO_SPLIT,
          .by=COMBO_SPLIT) %>% 
  as_tsibble(key=COMBO_SPLIT,index=CAL_PERIOD)

# Calculate percent of zero demand periods for each combo
zeros <- df.ts.good %>%
  union(df.ts.bad) %>% 
  as_tibble() %>% 
  group_by(COMBO_SPLIT) %>% 
  filter(UNITS==0) %>% 
  summarise(n_zero=n())

df.ts.good %<>% 
  add_count(COMBO_SPLIT) %>%
  left_join.(zeros,by="COMBO_SPLIT") %>% 
  mutate.(pct_zero = 100*(n_zero/n))

df.ts.bad %<>% 
  add_count(COMBO_SPLIT) %>%
  left_join.(zeros,by="COMBO_SPLIT") %>% 
  mutate.(pct_zero = 100*(n_zero/n))

knitr::kable(df.ts.good %>% 
               summarise.(N_combos = n_distinct(COMBO_SPLIT),
                          .by=DEMAND_CATEGORY),caption="Number of combos that ARE NOT white noise by demand category") %>%
  kable_styling(full_width = F)
```

Every demand category had a lot of combos that were white noise.
In any given category, about 2/3 to 3/4's of the combos were white noise.

## Analyzing white noise combos

One particular issue for time-series forecasting is dealing with "count" data, which can often have a value of zero, as we've seen in the Bridges dataset.
I created a training set consisting of 90% of the bad combo data, and a test set of the remaining 10%.
I then fit the only models that make sense for a white-noise time series, which is the grand mean and the naive model.

```{r model fitting, echo=TRUE, message=FALSE, warning=FALSE}
# Make a tsibble again
df.ts.bad %<>% as_tsibble(key=COMBO_SPLIT,index=CAL_PERIOD)

# Create training set
train.bad <- df.ts.bad %>%
  group_by(COMBO_SPLIT) %>%
  slice_head(prop=0.9) %>% 
  as_tsibble(key=COMBO_SPLIT,index=CAL_PERIOD)

# Fit all models described above, this command automatically chooses the parameters for each kind of model based on lowest model fitment criteria scores
plan(multisession)

fit <- train.bad %>% 
  group_by(COMBO_SPLIT) %>% 
  model(
    naive = NAIVE(UNITS),
    snaive = SNAIVE(UNITS),
    mean = MEAN(UNITS),
  ) %>%
  mutate(combination = (naive + snaive + mean)/3)

# Forecast test data
test.bad <- df.ts.bad %>% 
  group_by(COMBO_SPLIT) %>%
  anti_join(train.bad) %>% 
  as_tsibble(key=COMBO_SPLIT,index=CAL_PERIOD)

plan(multisession)

fc.bad <- fit %>% 
  forecast(test.bad)

# Calculate test accuracy
test.acc.bad <- accuracy(fc.bad,df.ts.bad)
```

After fitting the different models to each combo, I then filtered the results to display the most accurate model on the test data from the white-noise combos.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Filter and display best fitting models
best.test.bad <- test.acc.bad %>% 
  filter.(MASE==min(MASE),
          .by=COMBO_SPLIT)

lowest.mase.bad <- best.test.bad %>% 
  group_by(.model) %>% 
  summarize(n=n(),avg.MASE=mean(MASE)) %>% 
  arrange(-n)

knitr::kable(lowest.mase.bad) %>%
  kable_styling(full_width = F)
```

These forecasts represent the worst case scenario - if a client wants a forecast for a combo that is just white noise, then fitting a naive, seasonal naive, and grand mean model are your options.
Though there is the caveat that forecasts of the naive model simply use the last time point available as the future prediction, which is of questionable value.

On that point, it's important to note that the naive/seasonal naive models will sometimes achieve "perfect" accuracy by predicting all test data will be 0, whereas the mean/combination model will never predict all 0's.
If a client really does want a forecast for a product like this, they should be cautioned that the "perfect" accuracy (MASE=0) is only achieved by predicting that the product will never sell again (which is likely an unrealistic prediction).

I selected a combo to plot that demonstrates this point:

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Plot original values and then overlay the fitted values on top from each of the models
random <- sample(df.ts.bad$COMBO_SPLIT,1)

df.ts.bad %>% 
  filter(COMBO_SPLIT==111) %>% 
  autoplot(UNITS) +
  autolayer(fc.bad %>% filter(COMBO_SPLIT==111),level=NULL,alpha=0.5)
```

## Analyzing the non-random but intermittent combos

The most commonly applied analysis for intermittent demand time-series data is Croston's method, which I will perform below as it is built-in to the R package `fable`.

Croston's method takes a time-series and decomposes it into two new time-series: one which contains the time periods that have non-zero demand, and one which notes the time between periods that contain non-zero demand.
It then performs separate exponential smoothing forecasts on them, and averages their predictions together to get the demand per time period.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Filter to combos with more than 20% of periods as zero
df.ts.good %<>% filter(pct_zero >= 20)

# Make tsibble again
df.ts.good %<>% as_tsibble(key=COMBO_SPLIT,index=CAL_PERIOD)

# Create training set
train.good <- df.ts.good %>%
  group_by(COMBO_SPLIT) %>% 
  slice_head(prop=0.8) %>% 
  as_tsibble(key=COMBO_SPLIT,index=CAL_PERIOD)

# Fit all models described above, this command automatically chooses the parameters for each kind of model based on lowest model fitment criteria scores
plan(multisession)

fit <- train.good %>% 
  group_by(COMBO_SPLIT) %>% 
  model(
    naive = NAIVE(UNITS),
    snaive = SNAIVE(UNITS),
    mean = MEAN(UNITS),
    croston = CROSTON(UNITS,type="sbj"),
  ) %>% 
  mutate(combination=(naive+snaive+croston)/3)

# Forecast test data
test.good <- df.ts.good %>% 
  group_by(COMBO_SPLIT) %>% 
  anti_join(train.good) %>% 
  as_tsibble(key=COMBO_SPLIT,index=CAL_PERIOD)

plan(multisession)

fc.good <- fit %>% 
  forecast(test.good)

# Calculate test accuracy
test.acc.good <- accuracy(fc.good,df.ts.good)
```

I then calculated the average MASE by model:

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Narrow down to the highest accuracy model for each COMBO_SPLIT using MASE scores 
best.test.good <- test.acc.good %>%
  filter.(MASE==min(MASE),
          .by=COMBO_SPLIT)

# Calculate the proportion of the data for which each type of model was most accurate
knitr::kable(best.test.good %>% 
  group_by(.model) %>% 
  summarize(n=n(),avg.MASE=mean(MASE)) %>% 
  arrange(-n) %>% 
  na.omit(),
  caption="Average MASE score for best fitting models on good intermittent-demand combos",
  digits=3) %>% 
  kable_styling(full_width = F,font_size = 18)
```

Looking at the accuracy, again we have the same situation where the naive/seasonal naive model has slightly inflated performance because it is the only model that will predict zero demand (all other methods are biased away from zero by their nature).
Therefore it achieves a MASE of 0 when the test data is all zeroes.
The mean forecast will never predict zeroes without negative demand values, and neither will Croston's.
I also changed the train/test split to 80/20 from 90/10 because the latter also may lead to inflated performance because of the possibility of having all of the test be zeroes with such a small test set.

Nevertheless, Croston's is still very effective, but it really should only be used when there is a substantial amount of zeroes in the data, as other methods will vastly outperform it otherwise.
For the combos where Croston's was the best fitting model, it improved the MASE score by `r signif(mean(croston.improvement$pct.improvement),2)` % compared to the second-best fitting model (either the mean or naive forecasts).

```{r message=FALSE, warning=FALSE, include=FALSE}
croston.best <- test.acc.good %>% 
  group_by(COMBO_SPLIT) %>% 
  filter(MASE==min(MASE)) %>%
  filter(.model=="croston")

croston.improvement <- test.acc.good %>% 
  filter(COMBO_SPLIT %in% croston.best$COMBO_SPLIT) %>% 
  group_by(COMBO_SPLIT) %>% 
  summarise(pct.improvement=1-(min(MASE)/sort(MASE)[2])) %>% 
  mutate(pct.improvement=round(pct.improvement*100,4))

mean(croston.improvement$pct.improvement)
```

## Conclusions

In my opinion, adding Croston's method (with the appropriate corrections) to Euclid Studio would only enhance its ability to handle intermittent demand combos.
There are [Python](https://github.com/Valdecy/pyID) and R libraries available that perform these analyses and require no additional or special preprocessing of the data (beyond what is already necessary for time-series analyses in either language).
A continuing challenge will be interpreting the accuracy of the naive model compared to Croston's when the dataset is relatively small.
Croston's is similar to the naive model in the sense that for future predictions, it is more of a strategy than a true forecasting method.
The naive model does not produce forecasts into the future, instead it in essence tells the client to continue to use the previous month's data as a prediction for the next month.
Croston's is similar in that it tells the client how much inventory to keep on hand for when demand *does* occur, but does not make specific predictions like ARIMA (for example) would that "you will sell X units at Y date".
