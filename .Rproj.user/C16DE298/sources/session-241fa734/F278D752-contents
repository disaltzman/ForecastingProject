---
title: "GXO Daily Forecast"
author: "David Saltzman"
date: November 5, 2022
output: 
  html_notebook:
    theme: readable
    toc: true
editor_options: 
  markdown: 
    wrap: sentence
---

```{r Load Libraries, message=FALSE, warning=FALSE, include=FALSE}
# Change settings to not use scientific notation
options(scipen = 20,future.rng.onMisuse="ignore")

# Load packages.
# Data manipulation.
library(tidyverse)
library(janitor)
library(here)
library(lubridate)
library(future)
library(magrittr)
library(reticulate)
library(kableExtra)
library(tsbox)

# Plots.
library(ggplot2)
library(patchwork)

# Analyses.
library(fpp3)
library(fable.prophet)
library(fable.tbats)
```

# Overview

I began by loading in and doing some quick exploration of the data, including counts of observations:

```{r Load and Process Data, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
i_am("GXO.Rmd")

# Read in data
df <- readxl::read_excel("BoeingSalesbyDay20221031.xlsx")

# Convert data column to Fable date type
df %<>% 
  mutate(PERIOD_DATE=ymd(PERIOD_DATE))

# Create tsibble
df.ts <- df %>% 
  as_tsibble(key=PRODUNIT_ID,index=PERIOD_DATE)

# Do count of how many times each PRODUNIT appears in data
df.ts %>% 
  as_tibble() %>% 
  count(PRODUNIT_ID) %>% 
  arrange(desc(n))
```

Each unit of work (`PRODUNIT_ID`) does not have observations for every day, meaning that it is not always marked as a 0 quantity if it did not occur that day.
That required me to fill the missing periods with zeroes, as forecasting models cannot be performed with missing periods.
However, there is the question of how many zeroes had to get added to remove the time gaps:

```{r Fill gaps, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
# Fill gaps
df.ts %<>%
  group_by(PRODUNIT_ID) %>% 
  fill_gaps(QTY=0.001)

# Change zeroes already in data to 0.001 for MAPE calculation
df.ts %<>% 
  mutate(QTY=ifelse(QTY==0,QTY+0.001,QTY))

# Count "zeroes"
df.ts %>% 
  as_tibble() %>% 
  group_by(PRODUNIT_ID) %>% 
  filter(QTY==0.001) %>% 
  count(QTY)
```

Some of the units of work (especially `KIT_ORDERS_SHIPPED_TD`) had a substantial amount of zeroes added to them (approximately 20% of the periods have zero demand).
This may not bode well for forecasting these algorithms.

Next I implemented the Ljung-Box Test; all of the QTY of work passed, meaning there is forecastable signal within the data.
However, it's also possible that the span of repeated zeroes in some of these is artificially inflating the autocorrelation values.

```{r Ljung-Box Test, message=FALSE, warning=FALSE, include=FALSE}

# Perform Ljung-Box test
lb <- df.ts %>% 
  as_tibble() %>% 
  group_by(PRODUNIT_ID) %>% 
  summarise(p.value=ljung_box(QTY,lag=28)[2])
```

## QueBIT 9-Tile Approach

I also ran QueBIT's 9-Tile category sorting on the data; the data is largely split by total demand, with most units of work having high variation and differing on total demand (A vs C categories).

```{r 9-Tile, echo=FALSE, message=FALSE, warning=FALSE}
source_python("../forecast_categories_v1.py")

df.ts %<>% 
  as_tibble() %>% 
  mutate(PERIOD_DATE=as.Date(PERIOD_DATE),
         OFFSET=as.numeric(PERIOD_DATE-today())) %>%
  rename(CAL_PERIOD=PERIOD_DATE,UNITS=QTY,COMBO_SPLIT=PRODUNIT_ID)
  
df.ts <- forecast_plots(df.ts)

kable(df.ts %>% 
  group_by(DEMAND_CATEGORY) %>%
  summarise(n=n_distinct(COMBO_SPLIT))) %>% 
  kable_styling(full_width = F)
```

## Visualize All Data

I first began by visualizing all of the units of work at once:

```{r echo=FALSE, fig.height=18, fig.width=18,dpi=400, paged.print=TRUE}
# Rename columns back to original data names
df.ts %<>% 
  rename(PERIOD_DATE=CAL_PERIOD,
         QTY=UNITS,
         PRODUNIT_ID=COMBO_SPLIT) %>% 
  mutate(PERIOD_DATE=as.Date(PERIOD_DATE)) %>% 
  as_tsibble(key=PRODUNIT_ID,index=PERIOD_DATE)

# Plot of all units of work
ggplot(df.ts,aes(x=PERIOD_DATE,y=QTY,group=PRODUNIT_ID)) + 
  geom_line() +
  facet_wrap(~PRODUNIT_ID,scales="free",ncol = 3) + 
  ggeasy::easy_all_text_size(14)
```

Looking at these figures, one unit of work, `KIT_ORDERS_SHIPPED_TD` , has a massive period of zeroes that is potentially problematic.
Most of the other units of work seem to be slowly trending downwards over time.

## Visualize PRODUNIT_ID Individually {.tabset}

I chose a few plots randomly to visualize in more detail.
These plots shows the time-series in the top panel, the autocorrelation plot on the bottom-left, and a seasonality plot on the bottom-right.
What is most useful to look at is the autocorrelation plot on the bottom-left.
There is strong weekly seasonality, as there is strong autocorrelation at each interval of 7 days.
This is promising for our forecasts.

### PICK-BULK

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Plot
df.ts %>% 
  filter(PRODUNIT_ID=="PICK-BULK") %>% 
  gg_tsdisplay(QTY) + ggtitle("PICK-BULK")
```

### CYCLE_COUNTS

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Plot
df.ts %>% 
  filter(PRODUNIT_ID=="CYCLE_COUNTS") %>% 
  gg_tsdisplay(QTY) + ggtitle("CYCLE_COUNTS")
```

### KIT_ORDERS_SHIPPED_TD

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Plot
df.ts %>% 
  filter(PRODUNIT_ID=="KIT_ORDERS_SHIPPED_TD") %>% 
  gg_tsdisplay(QTY) + ggtitle("KIT_ORDERS_SHIPPED_TD")
```

# Choosing Forecasting Models

Next I began the process of selecting which forecasting model to use to generate the 21-day forecasts.
I used a 80/20 training/testing split of the data from each unit of work, and fit a series of forecasting models to those.
Through some trial and error, I focused on modeling the weekly seasonality, i.e., the fact that there was a strong relationship between and given `QTY` on any given day and one week before that day.

```{r Fit models, echo=TRUE, message=FALSE, warning=FALSE}
# Create training set using data 80% of the data
train <- df.ts %>%
  group_by(PRODUNIT_ID) %>% 
  slice_head(prop=0.8) %>% 
  as_tsibble(key=PRODUNIT_ID,index=PERIOD_DATE)

# Define decomposition models
STLF_Euclid <- decomposition_model(
    STL(log(QTY)),
    ETS(season_adjust)
  )

STLF_Weekly <- decomposition_model(
  STL(log(QTY) ~ season(period = "week"),
      robust = TRUE),
  ETS(season_adjust))
  
# Fit models to the training data
plan(multisession)

test.models <- train %>% 
  model(
    Mean = MEAN(log(QTY)),
    Theta = THETA(log(QTY)),
    ETS = ETS(log(QTY)),
    Croston = CROSTON(QTY,type = "sbj"),
    Linear.Regression = TSLM(log(QTY)),
    ARIMA = ARIMA(log(QTY)),
    Neural = NNETAR(log(QTY)),
    Euclid = STLF_Euclid,
    Euclid_Weekly = STLF_Weekly,
    Prophet_Week = prophet(log(QTY)~season(period="week",3))) %>% 
  mutate(kitchen_sink_ensemble=(Mean + Theta + ETS + Croston + Linear.Regression + ARIMA + Neural + Prophet_Week + Euclid + Euclid_Multiple)/10,ProphetandNeural=(Prophet_Week + Neural)/2)

test.models <- train %>% 
  model(
    Mean = MEAN(log(QTY)),
    Theta = THETA(log(QTY)),
    ETS = ETS(log(QTY)),
    Croston = CROSTON(QTY,type = "sbj"),
    Linear.Regression = TSLM(log(QTY)),
    ARIMA = ARIMA(log(QTY)),
    Euclid = STLF_Euclid,
    Euclid_Weekly = STLF_Weekly,
    Prophet_Week = prophet(log(QTY)~season(period="week",3))) %>% 
  mutate(kitchen_sink_ensemble=(Mean + Theta + ETS + Croston + Linear.Regression + ARIMA + Prophet_Week + Euclid + Euclid_Weekly)/9)

# Forecast
plan(multisession,workers=14)

test <- df.ts %>%
  group_by(PRODUNIT_ID) %>% 
  anti_join(train)

fc.test <- test.models %>% 
  forecast(test)

# Check accuracy of the forecasts
test.acc <- accuracy(fc.test,df.ts)
```

I then narrowed the fitted models just to those that had the lowest MAPE score for each unit of work:

```{r model accuracy, echo=FALSE, message=FALSE, warning=FALSE}
best.model.acc <- test.acc %>%
  group_by(PRODUNIT_ID) %>% 
  filter(MASE==min(MASE))

best.model.acc %>% 
  select(.model,PRODUNIT_ID,MASE) %>%
  kbl() %>% kable_styling()
```

# 21 Day Forecast

I then took these best fitting models and generated a 21-day ahead forecast, which you can see plotted following the actual values below:

```{r Forecast, echo=FALSE, fig.height=18, fig.width=18,dpi=400, message=FALSE, warning=FALSE, paged.print=TRUE}
# Make list of best fitting models
best.fits <- left_join(best.model.acc,test.models) %>% 
  filter(MAPE==min(MAPE)) %>% 
  pivot_longer(cols=12:23,
               names_to = "modelname",
               values_to = "modelspecs") %>%
  filter(.model==modelname) %>% 
  select(.model,PRODUNIT_ID,modelspecs)

# Refit them with full data
plan(multisession)
test.models <- df.ts %>% 
  model(
    Croston = CROSTON(QTY,type = "sbj"),
    Neural = NNETAR(log(QTY)),
    Prophet_Week = prophet(log(QTY)~ season(period="week",3))) %>% 
  mutate(ProphetandNeural=(Prophet_Week + Neural)/2)

# Forecast 3 weeks in advance
plan(multisession,workers=12)
fc.21 <- test.models %>% 
  forecast(h=21)

# Filter down to forecasts just from best fitting models
fc.21 %<>% 
  semi_join(best.fits)

plot.subset <- df.ts %>% 
  group_by(PRODUNIT_ID) %>% 
  filter(PERIOD_DATE > "2022-09-01") %>% 
  as_tsibble(key=PRODUNIT_ID,index = PERIOD_DATE) %>% 
  ungroup()

autoplot(fc.21,plot.subset,level=NULL) + 
  facet_wrap(~PRODUNIT_ID,nrow=7,ncol=3,scales="free") +
  ggeasy::easy_all_text_size(24)
```

Below is the 21 day ahead forecast in an interactive table, but it is also attached as a CSV.
The 80% and 95% confidence intervals around the forecasted value are included as well.

```{r Forecast Table, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
# Rename columns and drop prediction intervals
fc.21 %<>% hilo() %>%
  unpack_hilo(c("80%","95%")) %>% 
  as_tibble() %>% 
  select(-QTY) %>% 
  rename(FORECAST=.mean)

# Write to CSV
write.csv(fc.21,file="GXO_21DAY_FORECAST.csv")

# Table
fc.21
```

# Recurrent Neural Network

```{r}
df_python <- as_tibble(df.ts)
```

#### Import libraries

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow
from numpy import array
from numpy import hstack
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import RNN, SimpleRNN
from keras.preprocessing.sequence import TimeseriesGenerator
from keras.layers import Dropout
from keras.optimizers import Adam
from keras.layers.core import Activation
from keras.callbacks import LambdaCallback
from sklearn.preprocessing import MinMaxScaler

# Move data to Python environment
df = pd.DataFrame(r.df_python)

# Convert date column
df['PERIOD_DATE'] = pd.to_datetime(df['PERIOD_DATE'])

# Convert combo column to character
df['PRODUNIT_ID'] = df['PRODUNIT_ID'].astype('str')

# Set index
df = df.set_index(['PERIOD_DATE'])

# Subset for testing purposes
df = df[df['PRODUNIT_ID'] == 'PICK-BULK'].reindex()
```

#### Create time-series features

```{python Create time features}
# Create time series features from date information
df_features = (
                df
                .assign(day = df.index.day)
                .assign(month = df.index.month)
                .assign(day_of_week = df.index.dayofweek)
                .assign(week_of_year = df.index.week)
              )
```

#### Partition data

```{python}
# Set number of test data points to be used during training period
number_of_test_data = 65

# Set number of held-out data for final evaluation of model performance
number_of_holdout_data = 65

# Set training data
number_of_training_data = len(df) - number_of_holdout_data - number_of_test_data

print ("total, train, test, holdout:", len(df), number_of_training_data, number_of_test_data, number_of_holdout_data)
```

```{python}
# Make data frames
datatrain = df_features[:number_of_training_data]

datatest = df_features[-(number_of_test_data+number_of_holdout_data):-number_of_holdout_data]

datahold = df_features[-number_of_holdout_data:]
```

#### Create tensors

```{python}
in_seq1 = array(datatrain['day'])
in_seq2 = array(datatrain['month'])
in_seq3 = array(datatrain['day_of_week'])
in_seq4 = array(datatrain['week_of_year'])
out_seq_train = array(datatrain['QTY'])

in_seq1 = in_seq1.reshape((len(in_seq1), 1))
in_seq2 = in_seq2.reshape((len(in_seq2), 1))
in_seq3 = in_seq3.reshape((len(in_seq3), 1))
in_seq4 = in_seq4.reshape((len(in_seq4), 1))
out_seq_train = out_seq_train.reshape((len(out_seq_train), 1))

datatrain_feed = hstack((in_seq1, in_seq2, in_seq3, in_seq4, out_seq_train))
```

```{python}
in_seq1 = array(datatest['day'])
in_seq2 = array(datatest['month'])
in_seq3 = array(datatest['day_of_week'])
in_seq4 = array(datatest['week_of_year'])
out_seq_test = array(datatest['QTY'])

in_seq1 = in_seq1.reshape((len(in_seq1), 1))
in_seq2 = in_seq2.reshape((len(in_seq2), 1))
in_seq3 = in_seq3.reshape((len(in_seq3), 1))
in_seq4 = in_seq4.reshape((len(in_seq4), 1))
out_seq_test = out_seq_test.reshape((len(out_seq_test), 1))

datatest_feed  = hstack((in_seq1, in_seq2, in_seq3, in_seq4, out_seq_test))
```

```{python}
in_seq1 = array(datahold['day'])
in_seq2 = array(datahold['month'])
in_seq3 = array(datahold['day_of_week'])
in_seq4 = array(datahold['week_of_year'])
out_seq_hold  = array(datahold['QTY'])

in_seq1 = in_seq1.reshape((len(in_seq1), 1))
in_seq2 = in_seq2.reshape((len(in_seq2), 1))
in_seq3 = in_seq3.reshape((len(in_seq3), 1))
in_seq4 = in_seq4.reshape((len(in_seq4), 1))
out_seq_hold = out_seq_hold.reshape((len(out_seq_hold ), 1))

datahold_feed = hstack((in_seq1, in_seq2, in_seq3, in_seq4, out_seq_hold))
```

#### Create chunks for training/testing the model

```{python}
n_features = datatrain_feed.shape[1]
n_input = 5

generator_train = TimeseriesGenerator(datatrain_feed, out_seq_train, length=n_input, batch_size=len(datatrain_feed))

generator_test = TimeseriesGenerator(datatest_feed, out_seq_test, length=n_input, batch_size=1)

generator_hold = TimeseriesGenerator(datahold_feed, out_seq_hold, length=n_input, batch_size=1)
```

#### Specify network parameters

```{python}
print("timesteps, features:", n_input, n_features)

model = Sequential()

model.add(SimpleRNN(4, activation='relu', input_shape=(n_input, n_features), return_sequences = False))
model.add(Dense(1, activation='relu'))

adam = Adam(learning_rate=0.001)
model.compile(optimizer=adam, loss='mse')
```

#### Model Summary

```{python}
model.summary()
```

#### Fit model

```{python}
score = model.fit(generator_train, epochs=300, validation_data=generator_test)
```

```{python}
losses = score.history['loss']
val_losses = score.history['val_loss']
plt.figure(figsize=(10,5))
plt.plot(losses, label="trainset")
plt.plot(val_losses, label="testset")
plt.legend(bbox_to_anchor=(1.05, 1), loc=2)
plt.show()
```

#### Test Results

```{python}
df_result = pd.DataFrame({'Actual' : [], 'Prediction' : []})

for i in range(len(generator_test)):
    x, y = generator_test[i]
    x_input = array(x).reshape((1, n_input, n_features))
    yhat = model.predict(x_input, verbose=0)
    df_result = df_result.append({'Actual': y[0][0], 'Prediction': yhat[0][0]}, ignore_index=True)
    
df_result['Diff'] = (df_result['Prediction'] - df_result['Actual'])
```

```{python}
mean = df_result['Actual'].mean()
mae = (df_result['Actual'] - df_result['Prediction']).abs().mean()

print("mean: ", mean)
print("mae:", mae)

# Calcualte MASE
from sktime.performance_metrics.forecasting import MeanAbsoluteScaledError
mase = MeanAbsoluteScaledError()

y_true = pd.Series(data=df_result['Actual'])
y_pred = pd.Series(data=df_result['Prediction'])
y_train = pd.Series(data=datatrain['QTY'])

print("MASE score:", mase(y_true=y_true,y_pred=y_pred,y_train=y_train))
```

```{python}
plt.figure(figsize=(15,10))
plt.plot(df_result['Actual'], color='blue')
plt.plot(df_result['Prediction'], color='red')
plt.show()
```

#### Hold Out Data Results

```{python}
df_result = pd.DataFrame({'Actual' : [], 'Prediction' : []})

for i in range(len(generator_hold)):
    x, y = generator_hold[i]
    x_input = array(x).reshape((1, n_input, n_features))
    yhat = model.predict(x_input, verbose=0)
    df_result = df_result.append({'Actual': y[0][0], 'Prediction': yhat[0][0]}, ignore_index=True)
```

```{python}
df_result['Diff'] = (df_result['Prediction'] - df_result['Actual'])
```

```{python}
mean = df_result['Actual'].mean()
mae = (df_result['Actual'] - df_result['Prediction']).abs().mean()

print("mean: ", mean)
print("mae:", mae)

# Calcualte MASE
from sktime.performance_metrics.forecasting import MeanAbsoluteScaledError
mase = MeanAbsoluteScaledError()

y_true = pd.Series(data=df_result['Actual'])
y_pred = pd.Series(data=df_result['Prediction'])
y_train = pd.Series(data=datatrain['QTY'])

print("MASE score:", mase(y_true=y_true,y_pred=y_pred,y_train=y_train))

r.df_result = df_result
```

```{python}
plt.figure(figsize=(15,10))
plt.plot(df_result['Actual'], color='blue')
plt.plot(df_result['Prediction'], color='red')
plt.show()
r.df_result = df_result
```

```{r}
# Plot prophet forecast
autoplot(test %>% filter(PRODUNIT_ID=="PICK-BULK"),level=NULL,color="black",size=1) + 
  autolayer(fc.test %>% 
           filter(PRODUNIT_ID=="PICK-BULK"&.model=="Prophet_Week"),color="red",size=1,level=NULL) +
  xlab("DATE") + 
  ggtitle("Prophet Model Weekly Seasonality")

df_result$Date <- test %>% 
  ungroup() %>%
  filter(PRODUNIT_ID=="PICK-BULK") %>% 
  select(PERIOD_DATE) %>% 
  slice_tail(n=60)

df_result %<>% 
  pivot_longer(cols=1:2,names_to="Type",values_to = "QTY")

ggplot(df_result,aes(x=Date$PERIOD_DATE,y=QTY,color=Type)) + 
  geom_line(size=1) +
  scale_color_manual(values=c("black","red")) +
  xlab("DATE") +
  ggtitle("Simple Recurrent Neural Network") +
  ggeasy::easy_text_size(14) + 
  ggeasy::easy_remove_legend()
```
