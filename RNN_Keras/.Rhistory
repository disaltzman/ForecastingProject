#| warning: false
#| echo: true
#| error: false
# Import libraries
import pandas as pd
import tensorflow as tf
import numpy as np
# Plotting libraries
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(rc={"figure.dpi":400, 'savefig.dpi':400})
# Import functions that will be needed
from numpy import array
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import RNN, SimpleRNN
from keras.preprocessing.sequence import TimeseriesGenerator
from keras.layers import Dropout
from keras.optimizers import Adam
from keras.layers.core import Activation
from keras.callbacks import EarlyStopping
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
# Set random seed
np.random.seed(401)
#| warning: false
#| echo: true
#| error: false
# Read in data
df = (
pd.read_csv("rnn_keras_data.csv")
.rename(columns={'PERIOD_DATE':'DATE',
'PRODUNIT_ID':'ID',
'QTY':'QUANTITY'})
.sort_values(by=['DATE','ID'])
.astype({'DATE': 'datetime64[ns]'})
)
# Remove whitespace, dashes, and commas from quantity
df['QUANTITY'] = (
df['QUANTITY']
.str.strip()
.replace({'-':'0',',':''},regex=True)
.astype(int)
)
# Count # of observations for each SKU
print(df['ID'].
value_counts().
sort_values(ascending=False))
#| warning: false
#| echo: true
#| error: false
# Fill in missing days
import janitor
df = df.complete(
{'DATE': lambda date: pd.date_range(date.min(), date.max())},
by = ['ID'],
fill_value=0,
sort = True)
# Count # of observations after
print(df['ID'].
value_counts().
sort_values(ascending=False))
# Set index
df = df.set_index(['DATE'])
# Subset for testing purposes
df = df[df['ID'] == 'ID1'].reindex()
# Drop ID column as it is no longer needed
df = df.drop('ID',axis=1)
#| warning: false
#| echo: true
#| error: false
# Create various temporal features
df_features = (df
.assign(day = df.index.isocalendar().day)
.assign(month = df.index.month)
.assign(day_of_week = df.index.weekday)
.assign(week_of_year = df.index.isocalendar().week)
)
#| warning: false
#| echo: true
#| error: false
# Define the sizes for training, testing, and holdout sets
train_size = 0.8  # 80% of data for training
test_size = 0.2  # 20% of data for testing (will be further split into holdout data )
# Split the data into a training and test set first
train_data, test_data = train_test_split(df_features, test_size=test_size, shuffle=False)
# Then split the remaining data 50/50 (i.e., the "rightmost" portion of the original data) into a test and holdout set
test_data, holdout_data = train_test_split(test_data, test_size=0.5, shuffle=False)
# The resulting data are now split into training, testing, and holdout sets
print("Training data size:", len(train_data))
print("Testing data size:", len(test_data))
print("Holdout data size:", len(holdout_data))
#| warning: false
#| echo: true
#| include: false
# After splitting the data, I standardized the output values (QUANTITY) in the training data, and then use the transformation derived from the training set to standardize the test and held out data. This is to prevent data leakage, which could occur if we standardized the data before splitting it.
# # Calculate values to standardize QUANTITY data
# scaler = MinMaxScaler()
#
# # Standardize QUANTITY in training data
# train_data['QUANTITY'] = scaler.fit_transform(train_data[['QUANTITY']])
#
# # Standardize QUANTITY in test data
# test_data['QUANTITY'] = scaler.transform(test_data[['QUANTITY']])
#
# # Standardize QUANTITY in held out data
# holdout_data['QUANTITY'] = scaler.transform(holdout_data[['QUANTITY']])
#| warning: false
#| echo: true
#| error: false
# Get number of features and lookback length
n_features = train_data.shape[1]
lookback_length = 5
# create training generator
train_generator = TimeseriesGenerator(
train_data.values.astype('float32'),
train_data.values[:,0].reshape((len(train_data.values), 1)).astype('float32'),
length=lookback_length,
batch_size=len(train_data)
)
# create test generator
test_generator = TimeseriesGenerator(
test_data.values.astype('float32'),
test_data.values[:,0].reshape((len(test_data.values), 1)).astype('float32'),
length=lookback_length,
batch_size=1
)
# create holdout generator
hold_generator = TimeseriesGenerator(
holdout_data.values.astype('float32'),
holdout_data.values[:,0].reshape((len(holdout_data.values), 1)).astype('float32'),
length=lookback_length,
batch_size=1
)
#| warning: false
#| echo: true
#| error: false
print("timesteps, features:", lookback_length, n_features)
# Initialize model
model = Sequential(name='simpleRNN_Model')
# Add recurrence to model
model.add(SimpleRNN(50, activation='relu', input_shape=(lookback_length, n_features), return_sequences = False))
# Add fully connected layer
model.add(Dense(1, activation='relu'))
# Define optimizer
adam = Adam(learning_rate=0.001)
# Register the custom metric function with the Keras model.
model.compile(loss='mse',optimizer='adam',metrics = ['mse', 'mae'])
# Summarize model we've created
model.summary()
#| warning: false
#| echo: true
#| error: false
# Define early stopping rule
early_stopping = EarlyStopping(monitor='val_loss',
patience=5,
mode='auto',
min_delta=0.1,
restore_best_weights=True,
verbose=1)
# Fit RNN
score = model.fit(train_generator,
epochs=1000,
validation_data=test_generator,
callbacks=[early_stopping],
verbose=0)
#| label: val_loss_fig
#| fig-cap: "Plot showing training and validation loss during model fitment"
#| warning: false
#| error: false
#| echo: false
# Get training + validation loss
losses = score.history['loss']
val_losses = score.history['val_loss']
# Create a figure and axes
fig, ax = plt.subplots()
sns.set_style("darkgrid")
# Plot training and validation losses
sns.lineplot(x=range(len(losses)), y=losses, ax=ax, label="Training Loss")
sns.lineplot(x=range(len(val_losses)), y=val_losses, ax=ax, label="Validation Loss",linestyle="--")
# Set the title and labels of plot
ax.set_title("Training and Validation Losses")
ax.set_xlabel("Epoch")
ax.set_ylabel("Loss")
# Show plot
plt.show()
reticulate::repl_python()
