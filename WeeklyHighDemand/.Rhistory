mutate(.model_desc = case_when(
grepl("TBATS",.model_desc)~"TBATS",
grepl("ARIMA",.model_desc)~"ARIMA",
grepl("NNAR",.model_desc)~"Feed For. NN",
grepl("ETS",.model_desc)~"ETS",
grepl("LM",.model_desc)~"Linear Regression",
TRUE ~ .model_desc
))
# Table
kable(best.mase %>%
group_by(.model_desc) %>%
summarize(n=n(),
avg.MASE=mean(mase),
med.MASE=median(mase)) %>%
arrange(-n),
col.names = c('Model', 'Num. of SKUs it was best for', 'Average MASE','Median MASE'),
caption="Average and Median MASE score for best fitting models") %>% kable_styling(full_width = F)
# Fit N-BEATS model
parallel_start(16)
model_fit_nbeats <- workflow_nbeats %>%
fit(data=training(splits))
# Fit DeepAR model
model_fit_deepAR <- workflow_deepAR %>%
fit(data=training(splits))
# Fit N-BEATS Ensemble model
model_fit_nbeats_ensemble <- workflow_nbeats_ensemble %>%
fit(data=training(splits))
# Fit feed-forward NN model
model_fit_nnetar <- workflow_nnetar %>%
fit(data=training(splits))
# Fit Prophet model
model_fit_prophet <- prophet_reg(seasonality_daily = F) %>%
set_engine("prophet") %>%
fit(gross_units ~ date, training(splits))
# Fit ARIMA model
model_fit_ARIMA <- arima_reg() %>%
set_engine("auto_arima") %>%
fit(gross_units ~ date, training(splits))
# Fit ETS model
model_fit_ETS <- exp_smoothing() %>%
set_engine("ets") %>%
fit(gross_units ~ date, training(splits))
# Fit Theta model
model_fit_theta <- exp_smoothing() %>%
set_engine("theta") %>%
fit(gross_units ~ date, training(splits))
# Model fit TBATS
model_fit_TBATS <- seasonal_reg() %>%
set_engine("tbats") %>%
fit(gross_units ~ date, training(splits))
# Model fit Linear Regression
model_fit_lingreg <- linear_reg() %>%
set_engine("lm") %>%
fit(gross_units ~ date, training(splits))
parallel_stop()
# Add all fitted models to a table so that operations can be performed on all models at once
models_tbl <- modeltime_table(
model_fit_nbeats,
model_fit_deepAR,
model_fit_nbeats_ensemble,
model_fit_nnetar,
model_fit_prophet,
model_fit_ARIMA,
model_fit_ETS,
model_fit_theta,
model_fit_TBATS,
model_fit_lingreg
)
# Calculate accuracy of models on testing data
calibration_tbl <- models_tbl %>%
modeltime_calibrate(
new_data = testing(splits),
id = "sku")
# Calculate accuracy on testing data
test.acc <- calibration_tbl %>%
modeltime_accuracy(acc_by_id = TRUE)
# Filter down to models with best MASE score by SKU
best.mase <- test.acc %>%
group_by(sku) %>%
filter(mase==min(mase)) %>%
mutate(.model_desc = case_when(
grepl("TBATS",.model_desc)~"TBATS",
grepl("ARIMA",.model_desc)~"ARIMA",
grepl("NNAR",.model_desc)~"Feed For. NN",
grepl("ETS",.model_desc)~"ETS",
grepl("LM",.model_desc)~"Linear Regression",
TRUE ~ .model_desc
))
# Table
kable(best.mase %>%
group_by(.model_desc) %>%
summarize(n=n(),
avg.MASE=mean(mase),
med.MASE=median(mase)) %>%
arrange(-n),
col.names = c('Model', 'Num. of SKUs it was best for', 'Average MASE','Median MASE'),
caption="Average and Median MASE score for best fitting models") %>% kable_styling(full_width = F)
# Fit N-BEATS model
parallel_start(16)
model_fit_nbeats <- workflow_nbeats %>%
fit(data=training(splits))
# Fit DeepAR model
model_fit_deepAR <- workflow_deepAR %>%
fit(data=training(splits))
# Fit N-BEATS Ensemble model
model_fit_nbeats_ensemble <- workflow_nbeats_ensemble %>%
fit(data=training(splits))
# Fit feed-forward NN model
model_fit_nnetar <- workflow_nnetar %>%
fit(data=training(splits))
# Fit Prophet model
model_fit_prophet <- prophet_reg(seasonality_daily = F) %>%
set_engine("prophet") %>%
fit(gross_units ~ date, training(splits))
# Fit ARIMA model
model_fit_ARIMA <- arima_reg() %>%
set_engine("auto_arima") %>%
fit(gross_units ~ date, training(splits))
# Fit ETS model
model_fit_ETS <- exp_smoothing() %>%
set_engine("ets") %>%
fit(gross_units ~ date, training(splits))
# Fit Theta model
model_fit_theta <- exp_smoothing() %>%
set_engine("theta") %>%
fit(gross_units ~ date, training(splits))
# Model fit TBATS
model_fit_TBATS <- seasonal_reg() %>%
set_engine("tbats") %>%
fit(gross_units ~ date, training(splits))
# Model fit Linear Regression
model_fit_lingreg <- linear_reg() %>%
set_engine("lm") %>%
fit(gross_units ~ date, training(splits))
parallel_stop()
# Add all fitted models to a table so that operations can be performed on all models at once
models_tbl <- modeltime_table(
model_fit_nbeats,
model_fit_deepAR,
model_fit_nbeats_ensemble,
model_fit_nnetar,
model_fit_prophet,
model_fit_ARIMA,
model_fit_ETS,
model_fit_theta,
model_fit_TBATS,
model_fit_lingreg
)
# Calculate accuracy of models on testing data
calibration_tbl <- models_tbl %>%
modeltime_calibrate(
new_data = testing(splits),
id = "sku")
# Calculate accuracy on testing data
test.acc <- calibration_tbl %>%
modeltime_accuracy(acc_by_id = TRUE)
# Filter down to models with best MASE score by SKU
best.mase <- test.acc %>%
group_by(sku) %>%
filter(mase==min(mase)) %>%
mutate(.model_desc = case_when(
grepl("TBATS",.model_desc)~"TBATS",
grepl("ARIMA",.model_desc)~"ARIMA",
grepl("NNAR",.model_desc)~"Feed For. NN",
grepl("ETS",.model_desc)~"ETS",
grepl("LM",.model_desc)~"Linear Regression",
TRUE ~ .model_desc
))
# Table
kable(best.mase %>%
group_by(.model_desc) %>%
summarize(n=n(),
avg.MASE=mean(mase),
med.MASE=median(mase)) %>%
arrange(-n),
col.names = c('Model', 'Num. of SKUs it was best for', 'Average MASE','Median MASE'),
caption="Average and Median MASE score for best fitting models") %>% kable_styling(full_width = F)
plotdata <- best.mase %>%
group_by(.model_desc) %>%
summarize(n=n(),
avg.MASE=mean(mase),
med.MASE=median(mase)) %>%
pivot_longer(cols = avg.MASE:med.MASE,
names_to="criterion",
values_to = "value")
ggplot(plotdata,aes(x=reorder(.model_desc, -n),y=value,fill=criterion,alpha=n)) +
geom_bar(stat="summary",position="dodge") +
geom_hline(yintercept = 1,linetype="dashed") +
scale_fill_manual("Criterion",labels=c("Average MASE","Median MASE"),values=c("black","orange")) +
scale_alpha("# of SKU's") +
ggeasy::easy_rotate_labels(which="x",angle=90) +
ggeasy::easy_text_size(14) +
labs(x="Model",y="MASE score (lower is better)")
plotdata <- best.mase %>%
group_by(.model_desc) %>%
summarize(n=n(),
avg.MASE=mean(mase),
med.MASE=median(mase)) %>%
pivot_longer(cols = avg.MASE:med.MASE,
names_to="criterion",
values_to = "value")
ggplot(plotdata,aes(x=reorder(.model_desc, -n),y=value,fill=criterion,alpha=n)) +
geom_bar(stat="summary",position="dodge") +
geom_hline(yintercept = 1,linetype="dashed") +
scale_fill_manual("Criterion",labels=c("Average MASE","Median MASE"),values=c("black","orange")) +
scale_alpha("# of SKU's") +
ggeasy::easy_rotate_labels(which="x",angle=90) +
ggeasy::easy_text_size(14) +
labs(x="Model",y="MASE score (lower is better)")
# Calculate scaled MASE scored
best.mase %<>%
group_by(.model_desc) %>%
mutate(scaled_mase=scale(mase))
ggplot(best.mase,aes(x=reorder(.model_desc, -mase),y=mase,group=.model_desc,size=scaled_mase)) +
geom_jitter(width=0.20,alpha=0.3) +
scale_size("Normalized \nMASE score") +
labs(x="Model",y="MASE score (lower is better)") +
ggeasy::easy_all_text_size(14) +
ggeasy::easy_rotate_labels(which="x",angle=90) +
ggeasy::easy_remove_legend()
ggplot(best.mase,aes(x=reorder(.model_desc, mase),y=mase,group=.model_desc,size=scaled_mase)) +
geom_jitter(width=0.20,alpha=0.3) +
scale_size("Normalized \nMASE score") +
labs(x="Model",y="MASE score (lower is better)") +
ggeasy::easy_all_text_size(14) +
ggeasy::easy_rotate_labels(which="x",angle=90) +
ggeasy::easy_remove_legend()
ggplot(plotdata,aes(x=reorder(.model_desc, -value),y=value,fill=criterion,alpha=n)) +
geom_bar(stat="summary",position="dodge") +
geom_hline(yintercept = 1,linetype="dashed") +
scale_fill_manual("Criterion",labels=c("Average MASE","Median MASE"),values=c("black","orange")) +
scale_alpha("# of SKU's") +
ggeasy::easy_rotate_labels(which="x",angle=90) +
ggeasy::easy_text_size(14) +
labs(x="Model",y="MASE score (lower is better)")
ggplot(plotdata,aes(x=reorder(.model_desc, value),y=value,fill=criterion,alpha=n)) +
geom_bar(stat="summary",position="dodge") +
geom_hline(yintercept = 1,linetype="dashed") +
scale_fill_manual("Criterion",labels=c("Average MASE","Median MASE"),values=c("black","orange")) +
scale_alpha("# of SKU's") +
ggeasy::easy_rotate_labels(which="x",angle=90) +
ggeasy::easy_text_size(14) +
labs(x="Model",y="MASE score (lower is better)")
194/446
ggplot(best.mase,aes(x=reorder(.model_desc, mase),y=mase,group=.model_desc,size=scaled_mase)) +
geom_jitter(width=0.20,alpha=0.3) +
scale_size("Normalized \nMASE score") +
labs(x="Model",y="MASE score (lower is better)") +
ggeasy::easy_all_text_size(14) +
ggeasy::easy_rotate_labels(which="x",angle=90)
ggplot(best.mase,aes(x=reorder(.model_desc, mase),y=mase,group=.model_desc,size=scaled_mase)) +
geom_jitter(width=0.20,alpha=0.3) +
scale_size("Normalized \nMASE score") +
labs(x="Model",y="MASE score (lower is better)") +
ggeasy::easy_all_text_size(14) +
ggeasy::easy_rotate_labels(which="x",angle=90) +
ggeasy::easy_remove_legend()
save.image("C:/Users/disal/Desktop/GitHubRepos/ForecastingProject/WeeklyHighDemand/DeepLearningWeekly_env.RData")
reticulate::repl_python()
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow
import tensorflow
reticulate::repl_python()
import pandas as pd
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
import tensorflow
exit
py_install("tensorflow")
library(reticulate)
py_install("tensorflow")
reticulate::repl_python()
import tensorflow
from numpy import array
from numpy import hstack
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import RNN, SimpleRNN
from keras.preprocessing.sequence import TimeseriesGenerator
from keras.layers import Dropout
from keras.optimizers import Adam
from keras.layers.core import Activation
from keras.callbacks import LambdaCallback
from sklearn.preprocessing import MinMaxScaler
df = read_csv("keras_rnn_data.csv")
df = read.csv("keras_rnn_data.csv")
import pandas as pd
df = read_csv("keras_rnn_data.csv")
read_csv
df = read_csv("keras_rnn_data.csv")
# Fill in missing days
df = pd.read_csv("keras_rnn_data.csv")
View(df)
type(df)
df
class(df)
summary(df)
type(df)
df.dtypes()
pd.df.dtypes()
read_csv
df.dtypes
df['PERIOD_DATE'] = pd.to_datetime(df['PERIOD_DATE'])
df
df.dtype
df.dtypes
df['PRODUNIT_ID'] = pd.factor(df['PRODUNIT_ID'])
factorize
?factorize
?pd.factorize()
pd.factorize()
df['PRODUNIT_ID'] = pd.factorize(df['PRODUNIT_ID'])
df['PRODUNIT_ID']
df['PRODUNIT_ID'] = df['PRODUNIT_ID'].astype('str')
df.dtypes
df['QTY'] = pd.to_numeric(df['QTY'])
df['QTY'] = pd.to_numeric(df['QTY'])
df
df['QTY']
df['QTY']
df = pd.read_csv("keras_rnn_data.csv")
df['PERIOD_DATE'] = pd.to_datetime(df['PERIOD_DATE'])
# Convert to integer
df['QTY'] = pd.to_numeric(df['QTY'])
df['QTY'] = pd.to_numeric(df['QTY'])
df = pd.read_csv("keras_rnn_data.csv")
# Convert to date
df['PERIOD_DATE'] = pd.to_datetime(df['PERIOD_DATE'])
# Convert to integer
df['QTY'] = pd.to_numeric(df['QTY'])
df['QTY'] = df['QTY'].replace(',', '') # remove commas
df['QTY'] = pd.to_numeric(df['QTY'])
View(df)
df['QTY'] = df['QTY'].replace(',', '') # remove commas
df['QTY'] = df['QTY'].replace(',', '') # remove commas
df['QTY'] = pd.to_numeric(df['QTY'])
df['QTY']
df['QTY'] = df['QTY'].replace(',','') # remove commas
df['QTY'] = pd.to_numeric(df['QTY'])
df['QTY'] = df['QTY'].to_string().replace(',','') # remove commas
df['QTY'] = df['QTY'].to_string().replace(',','') # remove commas
df.dtypes
df.describe
df['QTY'] = df['QTY'].astype('str)replace(',','') # remove commas
df['QTY'] = df['QTY'].
df['QTY'] = df['QTY'].
astype('str').
replace(',','') # remove commas
df['QTY'] = df['QTY'].astype('str').replace(',','') # remove commas
df
df['PERIOD_DATE'] = pd.to_datetime(df['PERIOD_DATE'])
df
df['QTY'] = df['QTY'].astype('str').replace(',','') # remove commas
df['QTY'] = pd.to_numeric(df['QTY'])
df['PERIOD_DATE'] = pd.to_datetime(df['PERIOD_DATE'])
df['QTY'] = pd.to_numeric(df['QTY'],erros='coerce')
df['QTY'] = pd.to_numeric(df['QTY'],errors='coerce')
View(df)
df['QTY'] = pd.to_numeric(df['QTY'])
df = pd.read_csv("keras_rnn_data.csv")
df.dtypes
df['PERIOD_DATE'] = pd.to_datetime(df['PERIOD_DATE'])
df
df['PRODUNIT_ID'] = df.astype('str')
df['PRODUNIT_ID'] = df['PRODUNIT_ID'].astype('str')
df.dtypes
df = pd.read_csv("keras_rnn_data.csv")
df['PERIOD_DATE'] = df['PERIOD_DATE'].to_numeric()
df['PERIOD_DATE'] = df['PERIOD_DATE'].to_numeric()
df['PERIOD_DATE'] = df['PERIOD_DATE'].to_numeric(df['PERIOD_DATE'])
df['PERIOD_DATE'] = df['PERIOD_DATE'].pd.to_numeric(df['PERIOD_DATE'])
df['PERIOD_DATE'] = pd.to_numeric(df['PERIOD_DATE'])
df['PERIOD_DATE'] = pd.to_datetime(df['PERIOD_DATE'])
df
df.dtypes
df =
df = (
df = (
pd.read_csv("keras_rnn_data.csv").
rename(colimns={'PERIOD_DATE':'DATE','PRODUNIT_ID':'ID','QTY':'QUANTITY'})
)
df = (
pd.read_csv("keras_rnn_data.csv").
rename(columns={'PERIOD_DATE':'DATE','PRODUNIT_ID':'ID','QTY':'QUANTITY'})
)
df
df = (
pd.read_csv("keras_rnn_data.csv").
rename(columns={'PERIOD_DATE':'DATE',
'PRODUNIT_ID':'ID',
'QTY':'QUANTITY'}).
astype('Date':'datetime64[ns]',
'ID':'str',
'QUANTITY':'str')
)
df = (
pd.read_csv("keras_rnn_data.csv").
rename(columns={'PERIOD_DATE':'DATE',
'PRODUNIT_ID':'ID',
'QTY':'QUANTITY'}).
astype({'Date':'datetime64[ns]',
'ID':'str',
'QUANTITY':'str'})
)
df = (
pd.read_csv("keras_rnn_data.csv").
rename(columns={'PERIOD_DATE':'DATE',
'PRODUNIT_ID':'ID',
'QTY':'QUANTITY'}).
astype({'DATE':'datetime64[ns]',
'ID':'str',
'QUANTITY':'str'})
)
df
df.dtypes
df = (
pd.read_csv("keras_rnn_data.csv").
rename(columns={'PERIOD_DATE':'DATE',
'PRODUNIT_ID':'ID',
'QTY':'QUANTITY'}).
astype({'DATE':'datetime64[ns]'}).
assign(QUANTITY = df['QUANTITY'].replace(',',''))
)
View(df)
df.resample('1D')
df
df.resample('1D').mean()
df.resample('1D').mean().head()
foo = df.resample('1D').mean()
foo
df.head()
df.resample('1D')
df.resample('1D',on='DATE').mean()
df.resample('1D',on='DATE')
df.resample('1D',on='DATE').mean()
df.resample('1D',on='DATE').mean('DATE')
df.resample('1D',on='DATE').mean()
df.resample('1D').mean()
df.resample('1D').mean()
View(df)
View(df)
df['QUANTITY'] = df['QUANTITY'].replace('-','0')
View(df)
df['QUANTITY'] = df['QUANTITY'].str.strip()
View(df)
df['QUANTITY'] = df['QUANTITY'].replace('-','0')
View(df)
df = (
pd.read_csv("keras_rnn_data.csv").
rename(columns={'PERIOD_DATE':'DATE',
'PRODUNIT_ID':'ID',
'QTY':'QUANTITY'}).
astype({'DATE':'datetime64[ns]'})).
sort_values(by='DATE', ascending=False)
df = (
pd.read_csv("keras_rnn_data.csv").
rename(columns={'PERIOD_DATE':'DATE',
'PRODUNIT_ID':'ID',
'QTY':'QUANTITY'}).
astype({'DATE':'datetime64[ns]'})).
sort_values(by='DATE', ascending=False)
)
df = (
pd.read_csv("keras_rnn_data.csv").
rename(columns={'PERIOD_DATE':'DATE',
'PRODUNIT_ID':'ID',
'QTY':'QUANTITY'}).
astype({'DATE':'datetime64[ns]'}).
sort_values(by='DATE', ascending=False)
)
df
df = (
pd.read_csv("keras_rnn_data.csv").
rename(columns={'PERIOD_DATE':'DATE',
'PRODUNIT_ID':'ID',
'QTY':'QUANTITY'}).
astype({'DATE':'datetime64[ns]'}).
sort_values(by=['DATE','ID'], ascending=False)
)
df
View(df)
# Read in data
df = (
pd.read_csv("keras_rnn_data.csv").
rename(columns={'PERIOD_DATE':'DATE',
'PRODUNIT_ID':'ID',
'QTY':'QUANTITY'}).
astype({'DATE':'datetime64[ns]'}).
sort_values(by=['DATE','ID'])
)
df['QUANTITY'] = df['QUANTITY'].str.strip()
df['QUANTITY'] = df['QUANTITY'].replace('-','0')
# Fill in missing days
df.resample('1D').mean()
# Read in data
df = (
pd.read_csv("keras_rnn_data.csv").
rename(columns={'PERIOD_DATE':'DATE',
'PRODUNIT_ID':'ID',
'QTY':'QUANTITY'}).
astype({'DATE':'datetime64[ns]'}).
sort_values(by=['DATE','ID'])
)
df['QUANTITY'] = df['QUANTITY'].str.strip()
df['QUANTITY'] = df['QUANTITY'].replace('-','0')
# Fill in missing days
df.resample('1D').mean()
View(df)
View(df)
